{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2db92",
   "metadata": {},
   "source": [
    "# OCR Test with Tesseract\n",
    "\n",
    "## Requirements\n",
    "1. **Install Tesseract OCR**: \n",
    "   - Install Tesseract: `winget install --id UB-Mannheim.TesseractOCR` \n",
    "ion    - add it to system variables \n",
    "2. **Python packages**: \n",
    "   - `uv add opencv-python pytesseract pillow`\n",
    "   - `uv add nltk` (for advanced text correction)\n",
    "3. **Additional packages** (for AI correction):\n",
    "   - `uv add requests` (for API-based corrections)\n",
    "   - `uv add transformers torch` (for local model corrections - optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b24cd",
   "metadata": {},
   "source": [
    "## Only Pdf (textual format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4df974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./Tests output/output_from_MC test.txt\n",
      "\n",
      "\n",
      "Extracted text:\n",
      " Mobile Communication & Computing (MU-Sem. 7-Comp) 6-10 Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. TX (Transmit) points\n",
      "2. RX (Receive) Points\n",
      " A number of TX points provide coordinated transmission in the DL (DownLink).\n",
      " Similarly a number of RX points provide coordinated reception in the UL (UpLink).\n",
      " A TX/RX-point constitutes of a set of co-located TX/RX antennas providing coverage in the same sector.\n",
      " The set of TX/RX-points used in CoMP can either be at different locations, or co-sited but providing coverage in\n",
      "different sectors. They can also belong to the same or different eNBs.\n",
      " In Fig. 6.4.5 two simplified examples for DL CoMP is shown.\n",
      "(a)\n",
      "(b)\n",
      "Fig. 6.4.5 : Down Link CoMP (a) Joint Transmission (b) Dynamic point selection\n",
      " In both these cases Down link (DL) data is available for transmission from two TX-points. When two, or more,\n",
      " TX-points, transmit on the same frequency in the same subframe it is called Joint Transmission.\n",
      " When data is available for transmission at two or more TX-points but only scheduled from one TX-point in each\n",
      "subframe it is called Dynamic Point Selection.\n",
      " In case of Uplink (UL) CoMP, there is a Joint Reception i.e. a number of RX-points receive the UL data from one UE, and\n",
      "the received data is combined to improve the quality.\n",
      " When CoMP is used additional radio resources for signaling is required e.g. to provide UE scheduling information for\n",
      "the different DL/UL resources.\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-11 Long Term Evolution of 3GPP\n",
      "6.4.3 LTE Advanced Architecture\n",
      "6.4.3(a) Architecture\n",
      "Fig. 6.4.6 : LTE Advanced architecture\n",
      " The Fig. 6.4.6 depicts LTE Advanced (LTE-A) Architecture for E-UTRAN.\n",
      " It consists of P-GW, S-GW, MME, S1-MME, eNB, HeNB, HeNB-GW and Relay Node etc. Following are the functions of\n",
      "these architecture entities.\n",
      "P-GW\n",
      " It stands for PDN Gateway.\n",
      " It interfaced with S-GW using S5 interface and with operator's IP services using SGi interface.\n",
      " It has connectivity with PCRP using Gx interface.\n",
      " It connects UE to packet data networks.\n",
      " P-GW assigns IP address to the UE. One UE can have connectivity with more than one PGWs in order to have access to\n",
      "multiple PDNs.\n",
      " It takes care of packet filtering, policy enforcement and charging related services. Moreover it fulfils connectivity\n",
      "between 3GPP (LTE, LTE-A) and non 3GPP (WiMAX, CDMA etc.) technologies.\n",
      "S-GW\n",
      " It stands for Serving Gateway.\n",
      " It interfaces with MME using S11 interface and with SGSN using S4 interface.\n",
      " It connects with PDN-GW using S5 interface as mentioned above.\n",
      " EPC gets terminated at this node/entity. It is connected with E-UTRAN via S1-U interface.\n",
      " Each UE in LTE-A is associated to unique S-GW which has several functions.\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-12 Long Term Evolution of 3GPP\n",
      " It helps in inter-eNB handover as well as inter-3GPP mobility.\n",
      " It helps in inter-operator charging. It does packet routing and packet forwarding.\n",
      "MME\n",
      " It stands for Mobility Management Entity.\n",
      " It is major control plane element in LTE advanced architecture.\n",
      " It takes care of authentication, authorization and NAS signaling related security functions.\n",
      " It takes care of selecting either S-GW or PDN-GW or P-GW.\n",
      "S1-MME\n",
      "It provides connectivity between EPC and eNBs.\n",
      "eNB\n",
      " It is main building block or system in LTE-A.\n",
      " It provides interface with UEs or LTE-A phones.\n",
      " It has similar functionality as base station used in GSM or other cellular systems.\n",
      " Each of the eNBs serve one or several E-UTRAN cells. Interface between two eNBs is known as X2 interface.\n",
      "HeNB\n",
      " It stands for Home eNodeB or Home eNB.\n",
      " It is known as Fem to cell.\n",
      " It is used to improve coverage in the indoor region of office or home premises.\n",
      " It can be interfaced directly to EPC or via Gateway.\n",
      "HeNB-GW\n",
      " It provides connectivity of HeNB with S-GW and MME.\n",
      " It aggregates all the traffic from number of Home eNBs to core network.\n",
      " It uses S1 interface to connect with HeNBs.\n",
      "Relay Node :\n",
      "It is used for improving network performance.\n",
      "6.4.3(b) Comparison of LTE and LTE-A\n",
      " Both the LTE and LTE-Advanced are fourth generation wireless technologies designed to use for high speed broadband\n",
      "internet access.\n",
      " The specifications are published by 3rd Generation Partnership Project (3GPP).\n",
      " LTE is specified in 3GPP release 8 and LTE Advanced is specified in 3GPP release 10.\n",
      " LTE is the short form of Long Term Evolution. It uses FDD and TDD duplex modes for the UEs to communicate with the\n",
      "eNodeB. The LTE uses OFDMA modulation in the downlink (from eNodeB to UEs) and SC-FDMA modulation in the\n",
      "uplink. Various physical channels and logical channels are designed to take care of data as well as control information.\n",
      "It supports peak data rate of 300MBPS in the downlink and 75MBPS in the uplink (theoretically).\n",
      " TE-Advanced is the upgraded version of LTE technology to increase the peak data rates to about 1GBPS in the\n",
      "downlink and 500MBPS in the uplink. In order to increase the data rates LTE-Advanced utilizes higher number of\n",
      "antennas and added carrier aggregation feature.\n",
      " Table 6.4.2 summarizes the key differences between LTE and LTEA.\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-13 Long Term Evolution of 3GPP\n",
      "Table 6.4.2 : Difference between LTE and LTEA\n",
      "Specifications LTE LTE Advanced\n",
      "Standard 3GPP Release 9 3GPP Release 10\n",
      "Bandwidth Supports 1.4MHz, 3.0MHz, 5MHz, 10MHz, 70MHz Downlink(DL), 40MHz\n",
      "15MHz, 20MHz Uplink(UL)\n",
      "Data rate 300 Mbps Downlink(DL) 4x4MIMO and 20MHz, 1Gbps Downlink(DL), 500 Mbps\n",
      "75 Mbps Uplink(UL) Uplink(UL)\n",
      "Theoretical Throughput About 100Mbps for single chain 2 times than LTE\n",
      "(20MHz,100RB,64QAM), 400Mbps for 4x4\n",
      "MIMO. 25% of this is used for control/signaling\n",
      "(OVERHEAD)\n",
      "Maximum No. of Layers 2(category-3) and 4(category-4,5) in the 8 in the downlink, 4 in the uplink\n",
      "downlink, 1 in the uplink\n",
      "Maximum No. of 2 in the downlink, 1 in the uplink 2 in the downlink, 2 in the uplink\n",
      "codewords\n",
      "Spectral 16.3 for 4x4 MIMO in the downlink, 4.32 for 30 for 8x8 MIMO in the downlink, 15\n",
      "Efficiency(peak,b/s/Hz) 64QAM SISO case in the Uplink for 4x4 MIMO in the Uplink\n",
      "PUSCH and PUCCH Simultaneously not allowed Simultaneously allowed\n",
      "transmission\n",
      "Modulation schemes QPSK, 16QAM, 64QAM QPSK, 16QAM, 64QAM\n",
      "supported\n",
      "Access technique OFDMA (DL),DFTS-OFDM (UL) Hybrid OFDMA(DL), SC-FDMA(UL)\n",
      "Carrier aggregation Not supported Supported\n",
      "Applications Mobile broadband and VOIP Mobile broadband and VOIP\n",
      "6.4.4 LTE Protocol Stack\n",
      "Fig. 6.4.7 : LTE Protocol stack\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-14 Long Term Evolution of 3GPP\n",
      " Fig. 6.4.7 depicts LTE protocol stack. It is divided into two main parts - NAS (Non-Access Stratum) and AS (Access\n",
      "Stratum).\n",
      " Further it is categorized into control plane and user plane.\n",
      " User plane of eNB consists of PHY, MAC, RLC and PDCP layers.\n",
      " Control plane of eNB consists of these 4 layers and in addition RRC layer also.\n",
      "Following are functions of each layer.\n",
      "PHY\n",
      " This layer takes care of frame formation as per TDD or FDD topology and as per OFDMA structure.\n",
      " Moreover, it takes care of modulation and coding of different control and traffic channels.\n",
      " It covers scrambling and codeword to layer mapping functionalities.\n",
      " It incorporates reference signals which are used for channel estimation and channel equalization.\n",
      "MAC-Medium Access Control\n",
      "It takes care of following functions :\n",
      " Multiplexing/demultiplexing of RLC Packet Data Units (PDUs).\n",
      " Scheduling information reporting.\n",
      " Error correction through Hybrid ARQ (HARQ).\n",
      " Local Channel Prioritization.\n",
      " Padding.\n",
      "RLC-Radio Link Control\n",
      " Error correction through Automatic Repeat reQuest (ARQ).\n",
      " Segmentation according to the size of the transport block and re-segmentation in case a retransmission is needed.\n",
      " Concatenation of SDUs for the same radio bearer.\n",
      " Protocol error detection and recovery.\n",
      " In-sequence delivery.\n",
      "PDCP-Packet Data Convergence Protocol\n",
      " Header compression.\n",
      " In-sequence delivery and retransmission of PDCP Session Data Units (SDUs).\n",
      " Duplicate detection.\n",
      " Ciphering and integrity protection.\n",
      "RRC-Radio Resource Control\n",
      " Broadcast system information related to Non-Access Stratum (NAS) and Access Stratum (AS).\n",
      " Establishment, maintenance, and release of RRC connection.\n",
      " Security functions including key management.\n",
      " Mobility functions.\n",
      " QoS management functions.\n",
      " UE measurement reporting and control of the reporting.\n",
      " NAS direct message transfer between UE and NAS.\n",
      "NAS-Non Access Stratum\n",
      "Connection/session management between UE and the core network.\n",
      " Authentication.\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-15 Long Term Evolution of 3GPP\n",
      " Registration.\n",
      " Bearer context activation/deactivation.\n",
      " Location registration management.\n",
      "6.5 Higher Protocol Layers\n",
      "Higher layer protocols include :\n",
      "(i) Radio Link Control - RLC\n",
      "(ii) Packet Data Convergence Protocol - PDCP and\n",
      "(iii) Radio Resource Control - RRC\n",
      "6.5.1 Radio Link Control (RLC)\n",
      " RLC – Radio Link Control protocol is a data link layer protocol (Layer 2 protocol).\n",
      " An RLC entity receives/delivers RLC SDUs from/to upper layer and sends/receives RLC PDUs to/from its peer RLC entity\n",
      "via lower layers.\n",
      " If RLC entity configured at the eNB, there is a peer RLC entity configured at the UE and vice versa. (Fig 6.5.1 (b)) RLC\n",
      "performs following major functions :\n",
      "o Error correction through Automatic Repeat reQuest (ARQ).\n",
      "o Segmentation according to the size of the transport block and re-segmentation in case a retransmission is\n",
      "needed.\n",
      "o Concatenation of SDUs for the same radio bearer.\n",
      "o Protocol error detection and recovery.\n",
      "o In-sequence delivery.\n",
      "RLC Modes :\n",
      "An RLC entity can be configured to perform data transfer in one of the following three modes.\n",
      "1. Transparent Mode (TM)\n",
      " As the name suggests the Transparent mode entity in RLC does not add any overhead to the upper layer SDUs.\n",
      " The entity just transmits the SDUs coming from upper layer to MAC.\n",
      "In this mode :\n",
      "o Segmentation or reassembly of RLC SDUs is not allowed\n",
      "o No RLC headers are added.\n",
      "o Does not guarantees delivery\n",
      " RLC TM is used for transmission of paging messages on PCCH, system information transmitted on BCCH and SRB0\n",
      "messages transmitted on CCCH.\n",
      "2. Unacknowledged Mode (UM)\n",
      "RLC Unacknowledged Mode is used for transmission of delay sensitive packets, such as VoIP packets or audio/video\n",
      "streams.\n",
      "In this mode :\n",
      "o Segmentation or reassembly of RLC SDUs is allowed\n",
      "o RLC headers are added.\n",
      "o Does not guarantees delivery\n",
      "o This mode is suitable for carrying streaming traffic.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def extract_pdf(input_pdf: str, output_txt: str):\n",
    "    if not os.path.exists(input_pdf):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {input_pdf}')\n",
    "\n",
    "    out_dir = os.path.dirname(output_txt) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pages = []\n",
    "    with pdfplumber.open(input_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            pages.append(page.extract_text() or '')\n",
    "\n",
    "    text = '\\n'.join(pages).strip()\n",
    "\n",
    "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Defaults (change as needed)\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "output_txt = './Tests output/output_from_MC test.txt'\n",
    "\n",
    "# Run\n",
    "extracted_text = extract_pdf(input_pdf, output_txt)\n",
    "print(f'Saved to {output_txt}')\n",
    "print('\\n\\nExtracted text:\\n', extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ca817",
   "metadata": {},
   "source": [
    "## Only Image (clear Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e6c8df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not read image: ./Tests files/MC test.pdf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m output_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Tests output/output_from_MC test2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mrun_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_txt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved to ./Tests output/output_from_image.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracted text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,text)\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mrun_ocr\u001b[1;34m(input_path, output_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(input_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     17\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray, (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not read image: ./Tests files/MC test.pdf"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "def run_ocr(input_path: str, output_path: str):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f'Input file not found: {input_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    img = cv2.imread(input_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f'Could not read image: {input_path}')\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Defaults (change as needed)\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "output_txt = './Tests output/output_from_MC test2.txt'\n",
    "\n",
    "# Run\n",
    "text = run_ocr(input_pdf, output_txt)\n",
    "print('Saved to ./Tests output/output_from_image.txt')\n",
    "print('\\n\\nExtracted text:\\n',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d4e32",
   "metadata": {},
   "source": [
    "## Pdf (Mixed photos and text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2150179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text:\n",
      " [IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-10 \n",
      "Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. \n",
      "TX (Transmit) points\n",
      "2. \n",
      "RX (Receive) Points\n",
      " \n",
      "A number of TX points provide coordinated transmission in the DL (DownLink).\n",
      " \n",
      "Similarly a number of RX  points provide coordinated reception in the UL (UpLink).\n",
      " \n",
      "A TX/RX-point constitutes of a set of co-located TX/RX antennas providing coverage in the same sector.\n",
      " \n",
      "The set of TX/RX-points used in CoMP can either be at different locations, or co-sited but providing coverage in\n",
      "different sectors. They can also belong to the same or different eNBs.\n",
      " \n",
      "In Fig. 6.4.5 two simplified examples for DL CoMP is shown.\n",
      "[IMAGE OCR] SN, ee\n",
      "CTT\n",
      "\n",
      "Both the green and the blue TX-point\n",
      "transmits in each subframe\n",
      "(a)\n",
      "[IMAGE OCR] IX\n",
      "\n",
      "Radio frame, with 10 subframes aa\n",
      "\n",
      "The green and the blue TX-point\n",
      "transmits in different subframe\n",
      "(b)\n",
      "Fig. 6.4.5 : Down Link  CoMP  (a) Joint Transmission (b) Dynamic point selection\n",
      " \n",
      "In both these cases Down link (DL) data is available for transmission from two TX-points. When two, or more,\n",
      " \n",
      "TX-points, transmit on the same frequency in the same subframe it is called Joint Transmission.\n",
      " \n",
      "When data is available for transmission at two or more TX-points but only scheduled from one TX-point in each \n",
      "subframe it is called Dynamic Point Selection.\n",
      " \n",
      "In case of Uplink (UL) CoMP, there is a Joint Reception i.e. a number of RX-points receive the UL data from one UE, and \n",
      "the received data is combined to improve the quality.\n",
      " \n",
      "When CoMP is used additional radio resources for signaling is required e.g. to provide UE scheduling information for \n",
      "the different DL/UL resources.\n",
      "[IMAGE OCR] we ‘TechKnowledge\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-11 \n",
      "Long Term Evolution of 3GPP\n",
      "6.4.3 LTE Advanced Architecture\n",
      "6.4.3(a) Architecture\n",
      "[IMAGE OCR] EPC\n",
      "\n",
      "E-UTRAN\n",
      "[IMAGE OCR] Relay node\n",
      "Fig. 6.4.6  : LTE Advanced architecture\n",
      " \n",
      "The Fig. 6.4.6 depicts LTE Advanced (LTE-A) Architecture for E-UTRAN.\n",
      " \n",
      "It consists of P-GW, S-GW, MME, S1-MME, eNB, HeNB, HeNB-GW and Relay Node etc. Following are the functions of \n",
      "these architecture entities.\n",
      "P-GW\n",
      " \n",
      "It stands for PDN Gateway.\n",
      " \n",
      "It interfaced with S-GW using S5 interface and with operator's IP services using SGi interface.\n",
      " \n",
      "It has connectivity with PCRP using Gx interface.\n",
      " \n",
      "It connects UE to packet data networks.\n",
      " \n",
      "P-GW assigns IP address to the UE. One UE can have connectivity with more than one PGWs in order to have access to \n",
      "multiple PDNs.\n",
      " \n",
      "It takes care of packet filtering, policy enforcement and charging related services. Moreover it fulfils connectivity \n",
      "between 3GPP (LTE, LTE-A) and non 3GPP (WiMAX, CDMA etc.) technologies.\n",
      "S-GW\n",
      " \n",
      "It stands for Serving Gateway.\n",
      " \n",
      "It interfaces with MME using S11 interface and with SGSN using S4 interface.\n",
      " \n",
      "It connects with PDN-GW using S5 interface as mentioned above.\n",
      " \n",
      "EPC gets terminated at this node/entity. It is connected with E-UTRAN via S1-U interface.\n",
      " \n",
      "Each UE in LTE-A is associated to unique S-GW which has several functions.\n",
      "[IMAGE OCR] we ‘TechKnowledge\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-12 \n",
      "Long Term Evolution of 3GPP\n",
      " \n",
      "It helps in inter-eNB handover as well as inter-3GPP mobility.\n",
      " \n",
      "It helps in inter-operator charging. It does packet routing and packet forwarding.\n",
      "MME\n",
      " \n",
      "It stands for Mobility Management Entity.\n",
      " \n",
      "It is major control plane element in LTE advanced architecture.\n",
      " \n",
      "It takes care of authentication, authorization and NAS signaling related security functions.\n",
      " \n",
      "It takes care of selecting either S-GW or PDN-GW or P-GW.\n",
      "S1-MME\n",
      "It provides connectivity between EPC and eNBs.\n",
      "eNB\n",
      " \n",
      "It is main building block or system in LTE-A.\n",
      " \n",
      "It provides interface with UEs or LTE-A phones.\n",
      " \n",
      "It has similar functionality as base station used in GSM or other cellular systems.\n",
      " \n",
      "Each of the eNBs serve one or several E-UTRAN cells. Interface between two eNBs is known as X2 interface.\n",
      "HeNB\n",
      " \n",
      "It stands for Home eNodeB or Home eNB.\n",
      " \n",
      "It is known as Fem to cell.\n",
      " \n",
      "It is used to improve coverage in the indoor region of office or home premises.\n",
      " \n",
      "It can be interfaced directly to EPC or via Gateway.\n",
      "HeNB-GW\n",
      " \n",
      "It provides connectivity of HeNB with S-GW and MME.\n",
      " \n",
      "It aggregates all the traffic from number of Home eNBs to core network.\n",
      " \n",
      "It uses S1 interface to connect with HeNBs.\n",
      "Relay Node :\n",
      "It is used for improving network performance.\n",
      "6.4.3(b) \n",
      "Comparison of LTE and LTE-A\n",
      " \n",
      "Both the LTE and LTE-Advanced are fourth generation wireless technologies designed to use for high speed broadband \n",
      "internet access.\n",
      " \n",
      "The specifications are published by 3rd Generation Partnership Project (3GPP).\n",
      " \n",
      "LTE is specified in 3GPP release 8 and LTE Advanced is specified in 3GPP release 10.\n",
      " \n",
      "LTE is the short form of Long Term Evolution. It uses FDD and TDD duplex modes for the UEs to communicate with the \n",
      "eNodeB. The LTE uses OFDMA modulation in the downlink (from eNodeB to UEs) and SC-FDMA modulation in the \n",
      "uplink. Various physical channels and logical channels are designed to take care of data as well as control information. \n",
      "It supports peak data rate of 300MBPS in the downlink and 75MBPS in the uplink (theoretically).\n",
      " \n",
      "TE-Advanced is the upgraded version of LTE technology to increase the peak data rates to about 1GBPS in the \n",
      "downlink and 500MBPS in the uplink. In order to increase the data rates LTE-Advanced utilizes higher number of \n",
      "antennas and added carrier aggregation feature.\n",
      " \n",
      "Table 6.4.2 summarizes the key differences between LTE and LTEA.\n",
      "[IMAGE OCR] we ‘TechKnowledge\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-13 \n",
      "Long Term Evolution of 3GPP\n",
      "Table 6.4.2 : Difference between LTE and LTEA\n",
      "Specifications \n",
      "LTE \n",
      "LTE Advanced\n",
      "Standard \n",
      "3GPP Release 9 \n",
      "3GPP Release 10\n",
      "Bandwidth \n",
      "Supports 1.4MHz, 3.0MHz, 5MHz, 10MHz, \n",
      "15MHz, 20MHz \n",
      "70MHz Downlink(DL), 40MHz \n",
      "Uplink(UL)\n",
      "Data rate \n",
      "300 Mbps Downlink(DL) 4x4MIMO and 20MHz, \n",
      "75 Mbps Uplink(UL) \n",
      "1Gbps Downlink(DL), 500 Mbps \n",
      "Uplink(UL)\n",
      "Theoretical Throughput \n",
      "About 100Mbps for single chain \n",
      "(20MHz,100RB,64QAM), 400Mbps for 4x4 \n",
      "MIMO. 25% of this is used for control/signaling \n",
      "(OVERHEAD)\n",
      "2 times than LTE\n",
      "Maximum No. of Layers \n",
      "2(category-3) and 4(category-4,5) in the \n",
      "downlink, 1 in the uplink \n",
      "8 in the downlink, 4 in the uplink\n",
      "Maximum No. of \n",
      "codewords \n",
      "2 in the downlink, 1 in the uplink \n",
      "2 in the downlink, 2 in the uplink\n",
      "Spectral \n",
      "Efficiency(peak,b/s/Hz) \n",
      "16.3 for 4x4 MIMO in the downlink, 4.32 for \n",
      "64QAM SISO case in the Uplink \n",
      "30 for 8x8 MIMO in the downlink, 15 \n",
      "for 4x4 MIMO in the Uplink\n",
      "PUSCH and PUCCH \n",
      "transmission \n",
      "Simultaneously not allowed \n",
      "Simultaneously allowed\n",
      "Modulation schemes \n",
      "supported \n",
      "QPSK, 16QAM, 64QAM \n",
      "QPSK, 16QAM, 64QAM\n",
      "Access technique \n",
      "OFDMA (DL),DFTS-OFDM (UL) \n",
      "Hybrid OFDMA(DL), SC-FDMA(UL)\n",
      "Carrier aggregation \n",
      "Not supported \n",
      "Supported\n",
      "Applications \n",
      "Mobile broadband and VOIP \n",
      "Mobile broadband and VOIP\n",
      "6.4.4 LTE Protocol Stack\n",
      "[IMAGE OCR] Nonaccess | MME ‘| sew\n",
      "Stratum i\n",
      "(NAS) NAS i\n",
      "eNB\n",
      "RRC\n",
      "PDCP\n",
      "Access\n",
      "Stratum RLC\n",
      "(AS)\n",
      "MAC\n",
      "PHY\n",
      "\n",
      "Control\n",
      "plane\n",
      "\n",
      "User\n",
      "plane\n",
      "[IMAGE OCR] MME\n",
      "Fig. 6.4.7 : LTE Protocol stack\n",
      "[IMAGE OCR] we ‘TechKnowledge\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-14 \n",
      "Long Term Evolution of 3GPP\n",
      " \n",
      "Fig. 6.4.7 depicts LTE protocol stack. It is divided into two main parts - NAS (Non-Access Stratum) and AS (Access \n",
      "Stratum).\n",
      " \n",
      "Further it is categorized into control plane and user plane.\n",
      " \n",
      "User plane of eNB consists of PHY, MAC, RLC and PDCP layers.\n",
      " \n",
      "Control plane of eNB consists of these 4 layers and in addition RRC layer also.\n",
      "Following are functions of each layer.\n",
      "PHY\n",
      " \n",
      "This layer takes care of frame formation as per TDD or FDD topology and as per OFDMA structure.\n",
      " \n",
      "Moreover, it takes care of modulation and coding of different control and traffic channels.\n",
      " \n",
      "It covers scrambling and codeword to layer mapping functionalities.\n",
      " \n",
      "It incorporates reference signals which are used for channel estimation and channel equalization.\n",
      "MAC-Medium Access Control\n",
      "It takes care of following functions :\n",
      " \n",
      "Multiplexing/demultiplexing of RLC Packet Data Units (PDUs).\n",
      " \n",
      "Scheduling information reporting.\n",
      " \n",
      "Error correction through Hybrid ARQ (HARQ).\n",
      " \n",
      "Local Channel Prioritization.\n",
      " \n",
      "Padding.\n",
      "RLC-Radio Link Control\n",
      " \n",
      "Error correction through Automatic Repeat reQuest (ARQ).\n",
      " \n",
      "Segmentation according to the size of the transport block and re-segmentation in case a retransmission is needed.\n",
      " \n",
      "Concatenation of SDUs for the same radio bearer.\n",
      " \n",
      "Protocol error detection and recovery.\n",
      " \n",
      "In-sequence delivery.\n",
      "PDCP-Packet Data Convergence Protocol\n",
      " \n",
      "Header compression.\n",
      " \n",
      "In-sequence delivery and retransmission of PDCP Session Data Units (SDUs).\n",
      " \n",
      "Duplicate detection.\n",
      " \n",
      "Ciphering and integrity protection.\n",
      "RRC-Radio Resource Control\n",
      " \n",
      "Broadcast system information related to Non-Access Stratum (NAS) and Access Stratum (AS).\n",
      " \n",
      "Establishment, maintenance, and release of RRC connection.\n",
      " \n",
      "Security functions including key management.\n",
      " \n",
      "Mobility functions.\n",
      " \n",
      "QoS management functions.\n",
      " \n",
      "UE measurement reporting and control of the reporting.\n",
      " \n",
      "NAS direct message transfer between UE and NAS.\n",
      "NAS-Non Access Stratum\n",
      "Connection/session management between UE and the core network.\n",
      " \n",
      "Authentication.\n",
      "[IMAGE OCR] we ‘TechKnowledge\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] =\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-15 \n",
      "Long Term Evolution of 3GPP\n",
      " \n",
      "Registration.\n",
      " \n",
      "Bearer context activation/deactivation.\n",
      " \n",
      "Location registration management.\n",
      "6.5 \n",
      "Higher Protocol Layers\n",
      "Higher layer protocols include :\n",
      "(i) \n",
      "Radio Link Control - RLC\n",
      "(ii) Packet Data Convergence Protocol - PDCP and\n",
      "(iii) Radio Resource Control - RRC\n",
      "6.5.1 Radio Link Control (RLC)\n",
      " \n",
      "RLC – Radio Link Control protocol is a data link layer protocol (Layer 2 protocol). \n",
      " \n",
      "An RLC entity receives/delivers RLC SDUs from/to upper layer and sends/receives RLC PDUs to/from its peer RLC entity \n",
      "via lower layers. \n",
      " \n",
      "If RLC entity configured at the eNB, there is a peer RLC entity configured at the UE and vice versa. (Fig 6.5.1 (b)) RLC \n",
      "performs following major functions : \n",
      "o \n",
      "Error correction through Automatic Repeat reQuest (ARQ). \n",
      "o \n",
      "Segmentation according to the size of the transport block and re-segmentation in case a retransmission is \n",
      "needed. \n",
      "o \n",
      "Concatenation of SDUs for the same radio bearer. \n",
      "o \n",
      "Protocol error detection and recovery.  \n",
      "o \n",
      "In-sequence delivery.\n",
      "RLC Modes :\n",
      "An RLC entity can be configured to perform data transfer in one of the following three modes.\n",
      "1. \n",
      "Transparent Mode (TM)\n",
      " \n",
      "As the name suggests the Transparent mode entity in RLC does not add any overhead to the upper layer SDUs.\n",
      " \n",
      "The entity just transmits the SDUs coming from upper layer to MAC. \n",
      " \n",
      "In this mode :\n",
      "o \n",
      "Segmentation or reassembly of RLC SDUs is not allowed\n",
      "o \n",
      "No RLC headers are added.\n",
      "o \n",
      "Does not guarantees delivery\n",
      " \n",
      "RLC TM is used for transmission of paging messages on PCCH, system information transmitted on BCCH and SRB0 \n",
      "messages transmitted on CCCH.\n",
      "2. \n",
      "Unacknowledged Mode (UM)\n",
      "RLC Unacknowledged Mode is used for transmission of delay sensitive packets, such as VoIP packets or audio/video\n",
      "streams.\n",
      "In this mode :\n",
      "o \n",
      "Segmentation or reassembly of RLC SDUs is allowed\n",
      "o \n",
      "RLC headers are added.\n",
      "o \n",
      "Does not guarantees delivery\n",
      "o \n",
      "This mode is suitable for carrying streaming traffic.\n",
      "[IMAGE OCR] we ‘TechKnowledge\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_pdf_with_images(pdf_path, out_txt):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    final_text = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        blocks = []\n",
    "\n",
    "        # --- Text blocks (with bbox) ---\n",
    "        for b in page.get_text(\"blocks\"):\n",
    "            x0, y0, x1, y1, text, *_ = b\n",
    "            if text.strip():\n",
    "                blocks.append({\n",
    "                    \"bbox\": (x0, y0, x1, y1),\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": text.strip()\n",
    "                })\n",
    "\n",
    "        # --- Image blocks ---\n",
    "        raw_dict = page.get_text(\"rawdict\")\n",
    "        for block in raw_dict[\"blocks\"]:\n",
    "            if block[\"type\"] == 1:  # image\n",
    "                bbox = block[\"bbox\"]\n",
    "                img = page.get_pixmap(matrix=fitz.Matrix(2, 2), clip=fitz.Rect(bbox))\n",
    "                img_pil = Image.frombytes(\"RGB\", [img.width, img.height], img.samples)\n",
    "\n",
    "                # OCR on image\n",
    "                img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "                ocr_text = pytesseract.image_to_string(img_cv).strip()\n",
    "                if ocr_text:\n",
    "                    blocks.append({\n",
    "                        \"bbox\": bbox,\n",
    "                        \"type\": \"image\",\n",
    "                        \"content\": ocr_text\n",
    "                    })\n",
    "\n",
    "        # --- Sort by layout order ---\n",
    "        blocks.sort(key=lambda b: (round(b[\"bbox\"][1]), round(b[\"bbox\"][0])))\n",
    "\n",
    "        # --- Merge page ---\n",
    "        page_text = []\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == \"image\":\n",
    "                page_text.append(f\"[IMAGE OCR] {b['content']}\")\n",
    "            else:\n",
    "                page_text.append(b[\"content\"])\n",
    "        final_text.append(\"\\n\".join(page_text))\n",
    "\n",
    "    merged_text = \"\\n\\n--- Page Break ---\\n\\n\".join(final_text)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(merged_text)\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "\n",
    "# Example run\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "output_txt = './Tests output/output_from_MC test1.txt'\n",
    "\n",
    "text = extract_pdf_with_images(input_pdf, output_txt)\n",
    "print('Extracted text:\\n',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c03aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Hybrid Approach (pdfplumber + improved OCR)...\n",
      "Saved hybrid result to: ./Tests output/hybrid_output.txt\n",
      "Hybrid text preview:\n",
      "=== PAGE 1 - TEXT ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-10 Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. TX (Transmit) points\n",
      "2. RX (Receive) Points\n",
      " A number of TX points provide coordinated transmission in the ...\n",
      "\n",
      "\n",
      "Testing Advanced Layout Approach...\n",
      "Saved hybrid result to: ./Tests output/hybrid_output.txt\n",
      "Hybrid text preview:\n",
      "=== PAGE 1 - TEXT ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-10 Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. TX (Transmit) points\n",
      "2. RX (Receive) Points\n",
      " A number of TX points provide coordinated transmission in the ...\n",
      "\n",
      "\n",
      "Testing Advanced Layout Approach...\n",
      "Saved advanced result to: ./Tests output/advanced_layout_output.txt\n",
      "Advanced text preview:\n",
      "=== PAGE 1 ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-10 \n",
      "Long Term Evolution of 3GPP\n",
      "[IMAGE] 5.\n",
      "ae\n",
      "ae\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. \n",
      "TX (Transmit) points\n",
      "2. \n",
      "RX (Receive) Points\n",
      " \n",
      "A number of TX points provide coordinated trans...\n",
      "Saved advanced result to: ./Tests output/advanced_layout_output.txt\n",
      "Advanced text preview:\n",
      "=== PAGE 1 ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp)  6-10 \n",
      "Long Term Evolution of 3GPP\n",
      "[IMAGE] 5.\n",
      "ae\n",
      "ae\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. \n",
      "TX (Transmit) points\n",
      "2. \n",
      "RX (Receive) Points\n",
      " \n",
      "A number of TX points provide coordinated trans...\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def improved_ocr(image):\n",
    "    \"\"\"Enhanced OCR with better preprocessing\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "        \n",
    "    # Noise removal with bilateral filter (preserves edges)\n",
    "    filtered = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "    \n",
    "    # Otsu's thresholding for better binarization\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Morphological operations to clean up text\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    morph = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # OCR with custom configuration\n",
    "    custom_config = r'--oem 3 --psm 6 -l eng'\n",
    "    text = pytesseract.image_to_string(morph, config=custom_config)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_pdf_hybrid(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Hybrid approach: Use pdfplumber for native text and improved OCR for images\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {pdf_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    text_blocks = []\n",
    "    image_blocks = []\n",
    "    \n",
    "    # Step 1: Extract native text with pdfplumber (best for regular text)\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            extracted_text = page.extract_text() or ''\n",
    "            if extracted_text.strip():\n",
    "                text_blocks.append(f\"=== PAGE {page_num} - TEXT ===\\n{extracted_text}\")\n",
    "    \n",
    "    # Step 2: Extract and OCR images with PyMuPDF\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(pdf_doc, 1):\n",
    "        # Get all images on this page\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_idx, img_info in enumerate(image_list):\n",
    "            try:\n",
    "                xref = img_info[0]\n",
    "                base_img = pdf_doc.extract_image(xref)\n",
    "                image_bytes = base_img[\"image\"]\n",
    "                \n",
    "                # Convert to PIL Image\n",
    "                image_pil = Image.open(io.BytesIO(image_bytes))\n",
    "                image_np = np.array(image_pil)\n",
    "                \n",
    "                # Convert to OpenCV format\n",
    "                if len(image_np.shape) == 2:  # Grayscale\n",
    "                    image_cv = image_np\n",
    "                else:\n",
    "                    image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Apply improved OCR\n",
    "                image_text = improved_ocr(image_cv)\n",
    "                \n",
    "                if image_text.strip():\n",
    "                    image_blocks.append(f\"=== PAGE {page_num} - IMAGE {img_idx+1} ===\\n{image_text}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_idx+1} on page {page_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    \n",
    "    # Step 3: Combine all content\n",
    "    all_content = []\n",
    "    all_content.extend(text_blocks)\n",
    "    \n",
    "    if image_blocks:\n",
    "        all_content.append(\"\\n\" + \"=\"*50)\n",
    "        all_content.append(\"CONTENT FROM IMAGES\")\n",
    "        all_content.append(\"=\"*50)\n",
    "        all_content.extend(image_blocks)\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_text)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "def extract_pdf_advanced_layout(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Advanced layout-preserving extraction using PyMuPDF with improved OCR\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {pdf_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    final_text = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        blocks = []\n",
    "\n",
    "        # Get text blocks with bounding boxes\n",
    "        for b in page.get_text(\"blocks\"):\n",
    "            x0, y0, x1, y1, text, *_ = b\n",
    "            if text.strip():\n",
    "                blocks.append({\n",
    "                    \"bbox\": (x0, y0, x1, y1),\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": text.strip()\n",
    "                })\n",
    "\n",
    "        # Get image blocks with improved OCR\n",
    "        raw_dict = page.get_text(\"rawdict\")\n",
    "        for block in raw_dict[\"blocks\"]:\n",
    "            if block[\"type\"] == 1:  # image block\n",
    "                bbox = block[\"bbox\"]\n",
    "                # Higher resolution for better OCR\n",
    "                img = page.get_pixmap(matrix=fitz.Matrix(3, 3), clip=fitz.Rect(bbox))\n",
    "                img_pil = Image.frombytes(\"RGB\", [img.width, img.height], img.samples)\n",
    "\n",
    "                # Convert to OpenCV and apply improved OCR\n",
    "                img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "                ocr_text = improved_ocr(img_cv)\n",
    "                \n",
    "                if ocr_text:\n",
    "                    blocks.append({\n",
    "                        \"bbox\": bbox,\n",
    "                        \"type\": \"image\",\n",
    "                        \"content\": ocr_text\n",
    "                    })\n",
    "\n",
    "        # Sort blocks by reading order (top to bottom, left to right)\n",
    "        blocks.sort(key=lambda b: (round(b[\"bbox\"][1] / 10) * 10, round(b[\"bbox\"][0])))\n",
    "\n",
    "        # Format page content\n",
    "        page_content = []\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == \"image\":\n",
    "                page_content.append(f\"[IMAGE] {b['content']}\")\n",
    "            else:\n",
    "                page_content.append(b[\"content\"])\n",
    "        \n",
    "        final_text.append(f\"=== PAGE {page_num} ===\\n\" + \"\\n\".join(page_content))\n",
    "\n",
    "    doc.close()\n",
    "    \n",
    "    merged_text = \"\\n\\n\".join(final_text)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(merged_text)\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "# Test both approaches\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "\n",
    "print(\"Testing Hybrid Approach (pdfplumber + improved OCR)...\")\n",
    "hybrid_output = './Tests output/hybrid_output.txt'\n",
    "hybrid_text = extract_pdf_hybrid(input_pdf, hybrid_output)\n",
    "print(f\"Saved hybrid result to: {hybrid_output}\")\n",
    "print(f\"Hybrid text preview:\\n{hybrid_text[:500]}...\\n\")\n",
    "\n",
    "print(\"\\nTesting Advanced Layout Approach...\")\n",
    "advanced_output = './Tests output/advanced_layout_output.txt'\n",
    "advanced_text = extract_pdf_advanced_layout(input_pdf, advanced_output)\n",
    "print(f\"Saved advanced result to: {advanced_output}\")\n",
    "print(f\"Advanced text preview:\\n{advanced_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f88bbe",
   "metadata": {},
   "source": [
    "## Hybrid Approach with Advanced Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbf9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Enhanced Hybrid Approach with multiple OCR techniques...\n",
      "Extracting native text with pdfplumber...\n",
      "Processing images with enhanced OCR...\n",
      "Processing image 1 on page 1...\n",
      "Processing image 2 on page 1...\n",
      "Processing image 3 on page 1...\n",
      "Processing image 4 on page 1...\n",
      "Processing image 1 on page 2...\n",
      "Processing image 2 on page 2...\n",
      "Processing image 3 on page 2...\n",
      "Processing image 4 on page 2...\n",
      "Processing image 1 on page 3...\n",
      "Processing image 2 on page 3...\n",
      "Processing image 1 on page 4...\n",
      "Processing image 2 on page 4...\n",
      "Processing image 3 on page 4...\n",
      "Processing image 4 on page 4...\n",
      "Processing image 1 on page 5...\n",
      "Processing image 2 on page 5...\n",
      "Processing image 1 on page 6...\n",
      "Processing image 2 on page 6...\n",
      "\n",
      "Saved enhanced result to: ./Tests output/enhanced_hybrid_output.txt\n",
      "\n",
      "Enhanced text preview:\n",
      "=== PAGE 1 - TEXT ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-10 Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. TX (Transmit) points\n",
      "2. RX (Receive) Points\n",
      " A number of TX points provide coordinated transmission in the DL (DownLink).\n",
      " Similarly a number of RX points provide coordinated reception in the UL (UpLink).\n",
      " A TX/RX-point constitutes of a set of co-located TX/RX antennas providing coverage in the same sector.\n",
      " The set of TX/RX-points used in CoMP can either be at different locations, or co-sited but pro...\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import os\n",
    "\n",
    "def enhanced_image_preprocessing(image):\n",
    "    \"\"\"Multiple preprocessing techniques to handle different image types\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "    \n",
    "    # Technique 1: Standard preprocessing with bilateral filter\n",
    "    filtered = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "    _, binary1 = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    text1 = pytesseract.image_to_string(binary1, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "    if text1:\n",
    "        results.append((\"Standard\", text1))\n",
    "    \n",
    "    # Technique 2: Adaptive thresholding for varying lighting\n",
    "    adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    text2 = pytesseract.image_to_string(adaptive, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "    if text2:\n",
    "        results.append((\"Adaptive\", text2))\n",
    "    \n",
    "    # Technique 3: Morphological operations for noisy text\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    morph = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "    _, binary3 = cv2.threshold(morph, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    text3 = pytesseract.image_to_string(binary3, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "    if text3:\n",
    "        results.append((\"Morphological\", text3))\n",
    "    \n",
    "    # Technique 4: Contrast enhancement\n",
    "    enhanced = cv2.convertScaleAbs(gray, alpha=1.5, beta=30)\n",
    "    _, binary4 = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    text4 = pytesseract.image_to_string(binary4, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "    if text4:\n",
    "        results.append((\"Enhanced\", text4))\n",
    "    \n",
    "    # Technique 5: Different PSM modes for various text layouts\n",
    "    psm_modes = [3, 6, 8, 11, 13]  # Different page segmentation modes\n",
    "    for psm in psm_modes:\n",
    "        try:\n",
    "            text_psm = pytesseract.image_to_string(binary1, config=f'--oem 3 --psm {psm} -l eng').strip()\n",
    "            if text_psm and text_psm not in [r[1] for r in results]:\n",
    "                results.append((f\"PSM-{psm}\", text_psm))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def advanced_ocr_with_confidence(image):\n",
    "    \"\"\"OCR with confidence scoring to get the best result\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    filtered = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Get OCR data with confidence scores\n",
    "    try:\n",
    "        data = pytesseract.image_to_data(binary, output_type=pytesseract.Output.DICT, config=r'--oem 3 --psm 6 -l eng')\n",
    "        \n",
    "        # Filter words by confidence (keep words with confidence > 30)\n",
    "        high_conf_words = []\n",
    "        for i, word in enumerate(data['text']):\n",
    "            if int(data['conf'][i]) > 30 and word.strip():\n",
    "                high_conf_words.append(word)\n",
    "        \n",
    "        high_conf_text = ' '.join(high_conf_words)\n",
    "        \n",
    "        # Also get the full text for comparison\n",
    "        full_text = pytesseract.image_to_string(binary, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "        \n",
    "        # Return the longer/better text\n",
    "        if len(high_conf_text) > len(full_text) * 0.7:  # If high-confidence text is substantial\n",
    "            return high_conf_text\n",
    "        else:\n",
    "            return full_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Fallback to standard OCR\n",
    "        return pytesseract.image_to_string(binary, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "\n",
    "def extract_pdf_enhanced_hybrid(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid approach with multiple OCR techniques\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {pdf_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    text_blocks = []\n",
    "    image_blocks = []\n",
    "    \n",
    "    # Step 1: Extract native text with pdfplumber\n",
    "    print(\"Extracting native text with pdfplumber...\")\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            extracted_text = page.extract_text() or ''\n",
    "            if extracted_text.strip():\n",
    "                text_blocks.append(f\"=== PAGE {page_num} - TEXT ===\\n{extracted_text}\")\n",
    "    \n",
    "    # Step 2: Extract and OCR images with enhanced preprocessing\n",
    "    print(\"Processing images with enhanced OCR...\")\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(pdf_doc, 1):\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_idx, img_info in enumerate(image_list):\n",
    "            try:\n",
    "                print(f\"Processing image {img_idx+1} on page {page_num}...\")\n",
    "                xref = img_info[0]\n",
    "                base_img = pdf_doc.extract_image(xref)\n",
    "                image_bytes = base_img[\"image\"]\n",
    "                \n",
    "                # Convert to PIL Image and then to OpenCV\n",
    "                image_pil = Image.open(io.BytesIO(image_bytes))\n",
    "                \n",
    "                # Enhance image quality before OCR\n",
    "                if image_pil.mode != 'RGB':\n",
    "                    image_pil = image_pil.convert('RGB')\n",
    "                \n",
    "                # Apply PIL enhancements\n",
    "                enhancer = ImageEnhance.Contrast(image_pil)\n",
    "                image_pil = enhancer.enhance(1.5)\n",
    "                \n",
    "                enhancer = ImageEnhance.Sharpness(image_pil)\n",
    "                image_pil = enhancer.enhance(2.0)\n",
    "                \n",
    "                # Convert to OpenCV format\n",
    "                image_np = np.array(image_pil)\n",
    "                image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Try multiple OCR approaches\n",
    "                ocr_results = enhanced_image_preprocessing(image_cv)\n",
    "                \n",
    "                # Also try confidence-based OCR\n",
    "                conf_text = advanced_ocr_with_confidence(image_cv)\n",
    "                if conf_text:\n",
    "                    ocr_results.append((\"Confidence-based\", conf_text))\n",
    "                \n",
    "                # Combine and format results\n",
    "                if ocr_results:\n",
    "                    combined_text = []\n",
    "                    combined_text.append(f\"=== PAGE {page_num} - IMAGE {img_idx+1} ===\")\n",
    "                    \n",
    "                    # Find the best result (longest meaningful text)\n",
    "                    best_result = max(ocr_results, key=lambda x: len(x[1]) if len(x[1]) > 10 else 0)\n",
    "                    combined_text.append(f\"BEST RESULT ({best_result[0]}):\")\n",
    "                    combined_text.append(best_result[1])\n",
    "                    \n",
    "                    # Add other significant results\n",
    "                    other_results = [r for r in ocr_results if r != best_result and len(r[1]) > 5]\n",
    "                    if other_results:\n",
    "                        combined_text.append(\"\\nALTERNATIVE RESULTS:\")\n",
    "                        for method, text in other_results:\n",
    "                            if text != best_result[1]:  # Avoid duplicates\n",
    "                                combined_text.append(f\"- {method}: {text}\")\n",
    "                    \n",
    "                    image_blocks.append(\"\\n\".join(combined_text))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_idx+1} on page {page_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    \n",
    "    # Step 3: Combine all content\n",
    "    all_content = []\n",
    "    all_content.extend(text_blocks)\n",
    "    \n",
    "    if image_blocks:\n",
    "        all_content.append(\"\\n\" + \"=\"*60)\n",
    "        all_content.append(\"CONTENT FROM IMAGES (ENHANCED PROCESSING)\")\n",
    "        all_content.append(\"=\"*60)\n",
    "        all_content.extend(image_blocks)\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_text)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# Test the enhanced hybrid approach\n",
    "input_pdf = './Tests files/test_mix.pdf'\n",
    "enhanced_output = './Tests output/enhanced_hybrid_output.txt'\n",
    "\n",
    "print(\"Testing Enhanced Hybrid Approach with multiple OCR techniques...\")\n",
    "enhanced_text = extract_pdf_enhanced_hybrid(input_pdf, enhanced_output)\n",
    "print(f\"\\nSaved enhanced result to: {enhanced_output}\")\n",
    "print(f\"\\nEnhanced text preview:\\n{enhanced_text[:800]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444587b",
   "metadata": {},
   "source": [
    "## Rule Based OCR Text Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212e4b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Rule-Based Correction (No API needed) ===\n",
      "Extracting native text with pdfplumber...\n",
      "Processing images with enhanced OCR and AI correction...\n",
      "Processing image 1 on page 1...\n",
      "Processing image 2 on page 1...\n",
      "Processing image 3 on page 1...\n",
      "Processing image 4 on page 1...\n",
      "Processing image 1 on page 2...\n",
      "Processing image 2 on page 2...\n",
      "Processing image 3 on page 2...\n",
      "Processing image 4 on page 2...\n",
      "Processing image 1 on page 3...\n",
      "Processing image 2 on page 3...\n",
      "Processing image 1 on page 4...\n",
      "Processing image 2 on page 4...\n",
      "Processing image 3 on page 4...\n",
      "Processing image 4 on page 4...\n",
      "Processing image 1 on page 5...\n",
      "Processing image 2 on page 5...\n",
      "Processing image 1 on page 6...\n",
      "Processing image 2 on page 6...\n",
      "Saved to: ./Tests output/rule_based_corrected.txt\n",
      "Preview:\n",
      "=== PAGE 1 - TEXT ===\n",
      "Mobile Communication & Computing (MU-Sem. 7-Comp) 6-10 Long Term Evolution of 3GPP\n",
      "6.4.2(d) Coordinated Multipoint (CoMP)\n",
      "One of the key issues with many cellular systems is that of poor performance at the cell edges. To improve the\n",
      "performance at cell edges, LTE-Advanced introduces coordinated multipoint (CoMP) scheme.\n",
      "In CoMP there are two important components :\n",
      "1. TX (Transmit) points\n",
      "2. RX (Receive) Points\n",
      " A number of TX points provide coordinated transmission in the DL (DownLink).\n",
      " Similarly a number of RX points provide coordinated reception in the UL (UpLink).\n",
      "...\n",
      "\n",
      "\n",
      "=== Available Correction Methods ===\n",
      "1. rule_based - Offline regex-based corrections (recommended for start)\n",
      "2. openai - Uses OpenAI GPT for correction (requires API key)\n",
      "3. huggingface - Uses HF grammar correction models (requires API key)\n",
      "4. local - Downloads and uses local transformer model (requires transformers library)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "# Option 1: Using local transformers model (offline)\n",
    "def setup_local_text_correction():\n",
    "    \"\"\"Setup local model for OCR correction - requires transformers library\"\"\"\n",
    "    try:\n",
    "        from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        \n",
    "        # Use a grammar correction model (T5-based)\n",
    "        model_name = \"pszemraj/flan-t5-large-grammar-synthesis\"\n",
    "        # Alternative: \"vennify/t5-base-grammar-correction\"\n",
    "        \n",
    "        print(\"Loading grammar correction model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        corrector = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        return corrector\n",
    "    except ImportError:\n",
    "        print(\"transformers library not installed. Install with: pip install transformers torch\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Option 2: Using OpenAI API (requires API key)\n",
    "def correct_text_with_openai(text: str, api_key: str) -> str:\n",
    "    \"\"\"Correct OCR text using OpenAI API\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"Fix the OCR errors in this text. Correct spelling mistakes, fix spacing issues, and make the text readable while preserving the original meaning:\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Corrected text:\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'model': 'gpt-3.5-turbo',\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            'max_tokens': len(text) + 100,\n",
    "            'temperature': 0.1\n",
    "        }\n",
    "        \n",
    "        response = requests.post('https://api.openai.com/v1/chat/completions', \n",
    "                               headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content'].strip()\n",
    "        else:\n",
    "            print(f\"OpenAI API error: {response.status_code}\")\n",
    "            return text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with OpenAI correction: {e}\")\n",
    "        return text\n",
    "\n",
    "# Option 3: Rule-based OCR correction (offline, no dependencies)\n",
    "def rule_based_ocr_correction(text: str) -> str:\n",
    "    \"\"\"Apply common OCR error corrections using regex patterns\"\"\"\n",
    "    \n",
    "    corrections = [\n",
    "        # Common OCR character mistakes\n",
    "        (r'\\b0\\b', 'O'),  # Zero to O\n",
    "        (r'\\b1\\b', 'I'),  # One to I (in some contexts)\n",
    "        (r'rn', 'm'),     # rn often misread as m\n",
    "        (r'cl', 'd'),     # cl often misread as d\n",
    "        (r'vv', 'w'),     # vv often misread as w\n",
    "        (r'(\\w)1(\\w)', r'\\1l\\2'),  # 1 between letters often should be l\n",
    "        (r'(\\w)0(\\w)', r'\\1o\\2'),  # 0 between letters often should be o\n",
    "        \n",
    "        # Fix spacing issues\n",
    "        (r'([a-z])([A-Z])', r'\\1 \\2'),  # Add space between lowercase and uppercase\n",
    "        (r'([a-zA-Z])(\\d)', r'\\1 \\2'),  # Add space between letter and number\n",
    "        (r'(\\d)([a-zA-Z])', r'\\1 \\2'),  # Add space between number and letter\n",
    "        (r'\\s+', ' '),  # Multiple spaces to single space\n",
    "        \n",
    "        # Common word corrections\n",
    "        (r'\\btlle\\b', 'the'),\n",
    "        (r'\\btl1e\\b', 'the'),\n",
    "        (r'\\bfrom\\b', 'from'),\n",
    "        (r'\\bw1th\\b', 'with'),\n",
    "        (r'\\bth1s\\b', 'this'),\n",
    "        (r'\\bthat\\b', 'that'),\n",
    "        (r'\\bwh1ch\\b', 'which'),\n",
    "        (r'\\bwhere\\b', 'where'),\n",
    "        (r'\\bwhen\\b', 'when'),\n",
    "        (r'\\bhow\\b', 'how'),\n",
    "        (r'\\bwhy\\b', 'why'),\n",
    "        (r'\\bwhat\\b', 'what'),\n",
    "        \n",
    "        # Fix punctuation\n",
    "        (r'\\s+([,.!?;:])', r'\\1'),  # Remove space before punctuation\n",
    "        (r'([,.!?;:])\\s*([a-zA-Z])', r'\\1 \\2'),  # Ensure space after punctuation\n",
    "    ]\n",
    "    \n",
    "    corrected_text = text\n",
    "    for pattern, replacement in corrections:\n",
    "        corrected_text = re.sub(pattern, replacement, corrected_text)\n",
    "    \n",
    "    return corrected_text.strip()\n",
    "\n",
    "# Option 4: Using Hugging Face API (requires API key)\n",
    "def correct_text_with_huggingface(text: str, api_key: str) -> str:\n",
    "    \"\"\"Correct text using Hugging Face Inference API\"\"\"\n",
    "    try:\n",
    "        API_URL = \"https://api-inference.huggingface.co/models/pszemraj/flan-t5-large-grammar-synthesis\"\n",
    "        headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": f\"grammar: {text}\",\n",
    "            \"parameters\": {\"max_length\": len(text) + 50}\n",
    "        }\n",
    "        \n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return result[0].get('generated_text', text)\n",
    "        else:\n",
    "            print(f\"Hugging Face API error: {response.status_code}\")\n",
    "            \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Hugging Face correction: {e}\")\n",
    "        return text\n",
    "\n",
    "def extract_pdf_with_ai_correction(pdf_path, output_path, correction_method=\"rule_based\", api_key=None):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid extraction with AI-powered text correction\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {pdf_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup correction method\n",
    "    corrector = None\n",
    "    if correction_method == \"local\":\n",
    "        corrector = setup_local_text_correction()\n",
    "        if corrector is None:\n",
    "            print(\"Falling back to rule-based correction...\")\n",
    "            correction_method = \"rule_based\"\n",
    "    \n",
    "    text_blocks = []\n",
    "    image_blocks = []\n",
    "    \n",
    "    # Step 1: Extract native text with pdfplumber\n",
    "    print(\"Extracting native text with pdfplumber...\")\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            extracted_text = page.extract_text() or ''\n",
    "            if extracted_text.strip():\n",
    "                text_blocks.append(f\"=== PAGE {page_num} - TEXT ===\\n{extracted_text}\")\n",
    "    \n",
    "    # Step 2: Extract and OCR images with correction\n",
    "    print(\"Processing images with enhanced OCR and AI correction...\")\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(pdf_doc, 1):\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_idx, img_info in enumerate(image_list):\n",
    "            try:\n",
    "                print(f\"Processing image {img_idx+1} on page {page_num}...\")\n",
    "                xref = img_info[0]\n",
    "                base_img = pdf_doc.extract_image(xref)\n",
    "                image_bytes = base_img[\"image\"]\n",
    "                \n",
    "                # Enhanced image processing (from previous function)\n",
    "                image_pil = Image.open(io.BytesIO(image_bytes))\n",
    "                if image_pil.mode != 'RGB':\n",
    "                    image_pil = image_pil.convert('RGB')\n",
    "                \n",
    "                from PIL import ImageEnhance\n",
    "                enhancer = ImageEnhance.Contrast(image_pil)\n",
    "                image_pil = enhancer.enhance(1.5)\n",
    "                enhancer = ImageEnhance.Sharpness(image_pil)\n",
    "                image_pil = enhancer.enhance(2.0)\n",
    "                \n",
    "                image_np = np.array(image_pil)\n",
    "                image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Get raw OCR text\n",
    "                raw_ocr_text = pytesseract.image_to_string(image_cv, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "                \n",
    "                if raw_ocr_text:\n",
    "                    # Apply AI correction\n",
    "                    corrected_text = raw_ocr_text\n",
    "                    \n",
    "                    if correction_method == \"rule_based\":\n",
    "                        corrected_text = rule_based_ocr_correction(raw_ocr_text)\n",
    "                    elif correction_method == \"openai\" and api_key:\n",
    "                        corrected_text = correct_text_with_openai(raw_ocr_text, api_key)\n",
    "                    elif correction_method == \"huggingface\" and api_key:\n",
    "                        corrected_text = correct_text_with_huggingface(raw_ocr_text, api_key)\n",
    "                    elif correction_method == \"local\" and corrector:\n",
    "                        try:\n",
    "                            result = corrector(f\"grammar: {raw_ocr_text}\", max_length=len(raw_ocr_text) + 50)\n",
    "                            corrected_text = result[0]['generated_text'] if result else raw_ocr_text\n",
    "                        except:\n",
    "                            corrected_text = rule_based_ocr_correction(raw_ocr_text)\n",
    "                    \n",
    "                    # Format the result\n",
    "                    result_block = []\n",
    "                    result_block.append(f\"=== PAGE {page_num} - IMAGE {img_idx+1} ===\")\n",
    "                    result_block.append(\"RAW OCR:\")\n",
    "                    result_block.append(raw_ocr_text)\n",
    "                    result_block.append(f\"\\nCORRECTED ({correction_method.upper()}):\")\n",
    "                    result_block.append(corrected_text)\n",
    "                    \n",
    "                    image_blocks.append(\"\\n\".join(result_block))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_idx+1} on page {page_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    \n",
    "    # Step 3: Combine all content\n",
    "    all_content = []\n",
    "    all_content.extend(text_blocks)\n",
    "    \n",
    "    if image_blocks:\n",
    "        all_content.append(\"\\n\" + \"=\"*60)\n",
    "        all_content.append(\"CONTENT FROM IMAGES (WITH AI CORRECTION)\")\n",
    "        all_content.append(\"=\"*60)\n",
    "        all_content.extend(image_blocks)\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_text)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# Test different correction methods\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "\n",
    "print(\"=== Testing Rule-Based Correction (No API needed) ===\")\n",
    "rule_output = './Tests output/rule_based_corrected.txt'\n",
    "rule_text = extract_pdf_with_ai_correction(input_pdf, rule_output, \"rule_based\")\n",
    "print(f\"Saved to: {rule_output}\")\n",
    "print(f\"Preview:\\n{rule_text[:600]}...\\n\")\n",
    "\n",
    "# Uncomment below to test with API keys (you need to provide your own keys)\n",
    "\"\"\"\n",
    "print(\"=== Testing OpenAI Correction ===\")\n",
    "openai_api_key = \"your-openai-api-key-here\"\n",
    "openai_output = './Tests output/openai_corrected.txt'\n",
    "openai_text = extract_pdf_with_ai_correction(input_pdf, openai_output, \"openai\", openai_api_key)\n",
    "print(f\"Saved to: {openai_output}\")\n",
    "\n",
    "print(\"=== Testing Hugging Face Correction ===\")\n",
    "hf_api_key = \"your-huggingface-api-key-here\"\n",
    "hf_output = './Tests output/hf_corrected.txt'\n",
    "hf_text = extract_pdf_with_ai_correction(input_pdf, hf_output, \"huggingface\", hf_api_key)\n",
    "print(f\"Saved to: {hf_output}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Available Correction Methods ===\")\n",
    "print(\"1. rule_based - Offline regex-based corrections (recommended for start)\")\n",
    "print(\"2. openai - Uses OpenAI GPT for correction (requires API key)\")\n",
    "print(\"3. huggingface - Uses HF grammar correction models (requires API key)\")\n",
    "print(\"4. local - Downloads and uses local transformer model (requires transformers library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794b440",
   "metadata": {},
   "source": [
    "## NLTK-Based OCR Text Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addfe970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NLTK-Based OCR Correction...\n",
      "✓ Results saved to: ./Tests output/nltk_corrected.txt\n",
      "✓ Features: Spell checking, POS tagging, word frequency analysis\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pdfplumber\n",
    "import fitz\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# NLTK setup and correction functions\n",
    "def setup_nltk():\n",
    "    \"\"\"Setup required NLTK data with persistent storage\"\"\"\n",
    "    try:\n",
    "        # Set up NLTK data path for persistent storage\n",
    "        import os\n",
    "        nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "        if nltk_data_path not in nltk.data.path:\n",
    "            nltk.data.path.append(nltk_data_path)\n",
    "        \n",
    "        required_data = [\n",
    "            ('punkt', 'tokenizers'),\n",
    "            ('words', 'corpora'), \n",
    "            ('averaged_perceptron_tagger', 'taggers'),\n",
    "            ('brown', 'corpora')\n",
    "        ]\n",
    "        \n",
    "        missing_data = []\n",
    "        for data_name, data_type in required_data:\n",
    "            try:\n",
    "                nltk.data.find(f'{data_type}/{data_name}')\n",
    "                print(f\"✓ {data_name} already available\")\n",
    "            except LookupError:\n",
    "                missing_data.append(data_name)\n",
    "        \n",
    "        if missing_data:\n",
    "            print(f\"Downloading missing NLTK data: {', '.join(missing_data)}\")\n",
    "            for data_name in missing_data:\n",
    "                nltk.download(data_name, quiet=False)\n",
    "                print(f\"✓ Downloaded {data_name}\")\n",
    "            print(\"✓ All NLTK data now permanently installed\")\n",
    "        else:\n",
    "            print(\"✓ All required NLTK data already available\")\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"✗ NLTK not available\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error setting up NLTK: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_word_frequency_dict():\n",
    "    \"\"\"Create word frequency dictionary from NLTK corpora\"\"\"\n",
    "    try:\n",
    "        from nltk.corpus import brown, words\n",
    "        english_words = set(words.words())\n",
    "        brown_words = [word.lower() for word in brown.words() if word.isalpha()]\n",
    "        word_freq = Counter(brown_words)\n",
    "        return english_words, word_freq\n",
    "    except:\n",
    "        return set(), Counter()\n",
    "\n",
    "def nltk_spell_check(word, english_words, word_freq, max_distance=2):\n",
    "    \"\"\"Find best spelling correction for a word using edit distance\"\"\"\n",
    "    word = word.lower()\n",
    "    \n",
    "    # If word is already correct, return it\n",
    "    if word in english_words:\n",
    "        return word\n",
    "    \n",
    "    # Find words with similar spelling\n",
    "    candidates = []\n",
    "    for eng_word in english_words:\n",
    "        if abs(len(eng_word) - len(word)) <= max_distance:\n",
    "            distance = edit_distance(word, eng_word)\n",
    "            if distance <= max_distance:\n",
    "                # Use frequency as tie-breaker\n",
    "                frequency = word_freq.get(eng_word, 1)\n",
    "                candidates.append((eng_word, distance, frequency))\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by edit distance first, then by frequency (descending)\n",
    "        candidates.sort(key=lambda x: (x[1], -x[2]))\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    return word  # Return original if no candidates found\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"Calculate edit distance between two strings\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return edit_distance(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def nltk_ocr_correction(text):\n",
    "    \"\"\"Comprehensive OCR correction using NLTK\"\"\"\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "        english_words, word_freq = create_word_frequency_dict()\n",
    "        \n",
    "        if not english_words:\n",
    "            return rule_based_ocr_correction(text)\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            corrected_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                if word in string.punctuation:\n",
    "                    corrected_words.append(word)\n",
    "                    continue\n",
    "                \n",
    "                # Handle mixed alphanumeric OCR errors\n",
    "                corrected_word = word\n",
    "                if any(c.isdigit() for c in word) and any(c.isalpha() for c in word):\n",
    "                    corrected_word = re.sub(r'0', 'o', corrected_word)\n",
    "                    corrected_word = re.sub(r'1', 'l', corrected_word)\n",
    "                    corrected_word = re.sub(r'5', 'S', corrected_word)\n",
    "                    corrected_word = re.sub(r'8', 'B', corrected_word)\n",
    "                \n",
    "                # Apply spell checking\n",
    "                if corrected_word.isalpha() and len(corrected_word) > 1:\n",
    "                    spell_corrected = nltk_spell_check(corrected_word, english_words, word_freq)\n",
    "                    corrected_words.append(spell_corrected)\n",
    "                else:\n",
    "                    corrected_words.append(corrected_word)\n",
    "            \n",
    "            # Reconstruct sentence with proper spacing\n",
    "            corrected_sentence = ' '.join(corrected_words)\n",
    "            corrected_sentence = re.sub(r'\\s+([,.!?;:])', r'\\1', corrected_sentence)\n",
    "            corrected_sentence = re.sub(r'([,.!?;:])\\s*([A-Za-z])', r'\\1 \\2', corrected_sentence)\n",
    "            corrected_sentences.append(corrected_sentence)\n",
    "        \n",
    "        result = ' '.join(corrected_sentences)\n",
    "        result = re.sub(r'\\s+', ' ', result).strip()\n",
    "        return result\n",
    "        \n",
    "    except:\n",
    "        return rule_based_ocr_correction(text)\n",
    "\n",
    "def advanced_nltk_correction(text):\n",
    "    \"\"\"Advanced NLTK correction with context awareness\"\"\"\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "        from nltk.tag import pos_tag\n",
    "        from nltk.corpus import wordnet\n",
    "        \n",
    "        english_words, word_freq = create_word_frequency_dict()\n",
    "        if not english_words:\n",
    "            return nltk_ocr_correction(text)\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        corrected_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            pos_tags = pos_tag(words)\n",
    "            corrected_words = []\n",
    "            \n",
    "            for word, pos in pos_tags:\n",
    "                if word in string.punctuation:\n",
    "                    corrected_words.append(word)\n",
    "                    continue\n",
    "                \n",
    "                corrected_word = word\n",
    "                \n",
    "                # Context-aware OCR corrections\n",
    "                if pos.startswith('NN'):\n",
    "                    corrected_word = re.sub(r'rn', 'm', corrected_word)\n",
    "                    corrected_word = re.sub(r'cl', 'd', corrected_word)\n",
    "                elif pos.startswith('VB'):\n",
    "                    corrected_word = re.sub(r'1ng', 'ing', corrected_word)\n",
    "                elif pos.startswith('DT'):\n",
    "                    if corrected_word.lower() in ['tlle', 'tl1e']:\n",
    "                        corrected_word = 'the'\n",
    "                \n",
    "                # Apply spell checking\n",
    "                if corrected_word.isalpha() and len(corrected_word) > 1:\n",
    "                    spell_corrected = nltk_spell_check(corrected_word, english_words, word_freq)\n",
    "                    corrected_words.append(spell_corrected)\n",
    "                else:\n",
    "                    corrected_words.append(corrected_word)\n",
    "            \n",
    "            corrected_sentence = ' '.join(corrected_words)\n",
    "            corrected_sentence = re.sub(r'\\s+([,.!?;:])', r'\\1', corrected_sentence)\n",
    "            corrected_sentence = re.sub(r'([,.!?;:])\\s*([A-Za-z])', r'\\1 \\2', corrected_sentence)\n",
    "            corrected_sentences.append(corrected_sentence)\n",
    "        \n",
    "        result = ' '.join(corrected_sentences)\n",
    "        return re.sub(r'\\s+', ' ', result).strip()\n",
    "        \n",
    "    except:\n",
    "        return nltk_ocr_correction(text)\n",
    "\n",
    "# Fallback rule-based correction (from previous cell)\n",
    "def rule_based_ocr_correction(text: str) -> str:\n",
    "    \"\"\"Apply common OCR error corrections using regex patterns\"\"\"\n",
    "    corrections = [\n",
    "        (r'rn', 'm'), (r'cl', 'd'), (r'vv', 'w'),\n",
    "        (r'(\\w)1(\\w)', r'\\1l\\2'), (r'(\\w)0(\\w)', r'\\1o\\2'),\n",
    "        (r'([a-z])([A-Z])', r'\\1 \\2'), (r'\\s+', ' '),\n",
    "        (r'\\btlle\\b', 'the'), (r'\\btl1e\\b', 'the'),\n",
    "        (r'\\s+([,.!?;:])', r'\\1'), (r'([,.!?;:])\\s*([a-zA-Z])', r'\\1 \\2'),\n",
    "    ]\n",
    "    \n",
    "    corrected_text = text\n",
    "    for pattern, replacement in corrections:\n",
    "        corrected_text = re.sub(pattern, replacement, corrected_text)\n",
    "    \n",
    "    return corrected_text.strip()\n",
    "\n",
    "def extract_pdf_with_nltk_correction(pdf_path, output_path):\n",
    "    \"\"\"Enhanced hybrid extraction with NLTK-powered text correction\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {pdf_path}')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    \n",
    "    print(\"Setting up NLTK environment...\")\n",
    "    nltk_available = setup_nltk()\n",
    "    \n",
    "    if nltk_available:\n",
    "        print(\"✓ NLTK correction enabled\")\n",
    "    else:\n",
    "        print(\"✗ NLTK unavailable, using rule-based correction\")\n",
    "    \n",
    "    text_blocks = []\n",
    "    image_blocks = []\n",
    "    \n",
    "    # Extract native text\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            extracted_text = page.extract_text() or ''\n",
    "            if extracted_text.strip():\n",
    "                text_blocks.append(f\"=== PAGE {page_num} - TEXT ===\\n{extracted_text}\")\n",
    "    \n",
    "    # Extract and OCR images\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(pdf_doc, 1):\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_idx, img_info in enumerate(image_list):\n",
    "            try:\n",
    "                xref = img_info[0]\n",
    "                base_img = pdf_doc.extract_image(xref)\n",
    "                image_bytes = base_img[\"image\"]\n",
    "                \n",
    "                # Enhanced image processing\n",
    "                image_pil = Image.open(io.BytesIO(image_bytes))\n",
    "                if image_pil.mode != 'RGB':\n",
    "                    image_pil = image_pil.convert('RGB')\n",
    "                \n",
    "                enhancer = ImageEnhance.Contrast(image_pil)\n",
    "                image_pil = enhancer.enhance(1.5)\n",
    "                enhancer = ImageEnhance.Sharpness(image_pil)\n",
    "                image_pil = enhancer.enhance(2.0)\n",
    "                \n",
    "                image_np = np.array(image_pil)\n",
    "                image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # OCR and correction\n",
    "                raw_ocr_text = pytesseract.image_to_string(image_cv, config=r'--oem 3 --psm 6 -l eng').strip()\n",
    "                \n",
    "                if raw_ocr_text:\n",
    "                    corrected_text = (advanced_nltk_correction(raw_ocr_text) if nltk_available \n",
    "                                    else rule_based_ocr_correction(raw_ocr_text))\n",
    "                    \n",
    "                    result_block = [\n",
    "                        f\"=== PAGE {page_num} - IMAGE {img_idx+1} ===\",\n",
    "                        \"RAW OCR:\", raw_ocr_text,\n",
    "                        \"\\nCORRECTED (NLTK):\", corrected_text\n",
    "                    ]\n",
    "                    image_blocks.append(\"\\n\".join(result_block))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_idx+1} on page {page_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    \n",
    "    # Combine and save\n",
    "    all_content = text_blocks.copy()\n",
    "    if image_blocks:\n",
    "        all_content.extend([\"\\n\" + \"=\"*60, \"CONTENT FROM IMAGES (NLTK CORRECTED)\", \"=\"*60])\n",
    "        all_content.extend(image_blocks)\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(all_content)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_text)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# Test NLTK-based correction\n",
    "input_pdf = './Tests files/MC test.pdf'\n",
    "nltk_output = './Tests output/nltk_corrected.txt'\n",
    "\n",
    "print(\"Testing NLTK-Based OCR Correction...\")\n",
    "extract_pdf_with_nltk_correction(input_pdf, nltk_output)\n",
    "print(f\"✓ Results saved to: {nltk_output}\")\n",
    "print(\"✓ Features: Spell checking, POS tagging, word frequency analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Compact Version with Missing Features\n",
    "import os, io, re, string\n",
    "import pdfplumber, fitz, pytesseract, cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageEnhance\n",
    "import nltk\n",
    "\n",
    "# Setup NLTK with better feedback\n",
    "def setup_nltk():\n",
    "    try:\n",
    "        nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "        if nltk_data_path not in nltk.data.path:\n",
    "            nltk.data.path.append(nltk_data_path)\n",
    "        \n",
    "        required_data = [\n",
    "            ('punkt', 'tokenizers'),\n",
    "            ('words', 'corpora'), \n",
    "            ('averaged_perceptron_tagger', 'taggers'),\n",
    "            ('brown', 'corpora')\n",
    "        ]\n",
    "        \n",
    "        missing = []\n",
    "        for data_name, data_type in required_data:\n",
    "            try:\n",
    "                nltk.data.find(f'{data_type}/{data_name}')\n",
    "                print(f\"✓ {data_name} available\")\n",
    "            except LookupError:\n",
    "                missing.append(data_name)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"Downloading: {', '.join(missing)}\")\n",
    "            for data_name in missing:\n",
    "                nltk.download(data_name, download_dir=nltk_data_path, quiet=False)\n",
    "                print(f\"✓ Downloaded {data_name}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ NLTK setup failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Word dictionary + frequencies\n",
    "def create_dict():\n",
    "    try:\n",
    "        from nltk.corpus import brown, words\n",
    "        return set(words.words()), Counter(w.lower() for w in brown.words() if w.isalpha())\n",
    "    except:\n",
    "        return set(), Counter()\n",
    "\n",
    "# Enhanced edit distance\n",
    "def edit_distance(s1, s2):\n",
    "    if len(s1) < len(s2): return edit_distance(s2, s1)\n",
    "    if not s2: return len(s1)\n",
    "    prev = list(range(len(s2)+1))\n",
    "    for i,c1 in enumerate(s1):\n",
    "        curr = [i+1]\n",
    "        for j,c2 in enumerate(s2):\n",
    "            ins, dele, sub = prev[j+1]+1, curr[j]+1, prev[j]+(c1!=c2)\n",
    "            curr.append(min(ins,dele,sub))\n",
    "        prev = curr\n",
    "    return prev[-1]\n",
    "\n",
    "# Spell check with frequency ranking\n",
    "def spell_check(word, english, freq, max_d=2):\n",
    "    word = word.lower()\n",
    "    if word in english: return word\n",
    "    cands = []\n",
    "    for w in english:\n",
    "        if abs(len(w)-len(word)) <= max_d:\n",
    "            dist = edit_distance(word, w)\n",
    "            if dist <= max_d:\n",
    "                cands.append((w, dist, freq.get(w, 1)))\n",
    "    if cands: \n",
    "        return sorted(cands, key=lambda x: (x[1], -x[2]))[0][0]\n",
    "    return word\n",
    "\n",
    "# Advanced OCR correction with POS tagging\n",
    "def correct_text(text, english, freq):\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "        from nltk.tag import pos_tag\n",
    "        \n",
    "        out = []\n",
    "        for sent in sent_tokenize(text):\n",
    "            words = word_tokenize(sent)\n",
    "            pos_tags = pos_tag(words)\n",
    "            corrected = []\n",
    "            \n",
    "            for word, pos in pos_tags:\n",
    "                if word in string.punctuation: \n",
    "                    corrected.append(word)\n",
    "                    continue\n",
    "                \n",
    "                # Context-aware corrections based on POS\n",
    "                if pos.startswith('NN'):  # Nouns\n",
    "                    word = re.sub(r'rn', 'm', word)\n",
    "                    word = re.sub(r'cl', 'd', word)\n",
    "                elif pos.startswith('VB'):  # Verbs\n",
    "                    word = re.sub(r'1ng', 'ing', word)\n",
    "                elif pos.startswith('DT'):  # Determiners\n",
    "                    if word.lower() in ['tlle', 'tl1e']:\n",
    "                        word = 'the'\n",
    "                \n",
    "                # Common OCR fixes\n",
    "                if any(c.isdigit() for c in word) and any(c.isalpha() for c in word):\n",
    "                    word = re.sub(r'0', 'o', word)\n",
    "                    word = re.sub(r'1', 'l', word)\n",
    "                    word = re.sub(r'5', 'S', word)\n",
    "                    word = re.sub(r'8', 'B', word)\n",
    "                \n",
    "                # Spell check\n",
    "                if word.isalpha() and len(word) > 1:\n",
    "                    word = spell_check(word, english, freq)\n",
    "                \n",
    "                corrected.append(word)\n",
    "            \n",
    "            # Reconstruct sentence with proper spacing\n",
    "            s = ' '.join(corrected)\n",
    "            s = re.sub(r'\\s+([,.!?;:])', r'\\1', s)\n",
    "            s = re.sub(r'([,.!?;:])\\s*([A-Za-z])', r'\\1 \\2', s)\n",
    "            out.append(s)\n",
    "        \n",
    "        return re.sub(r'\\s+', ' ', ' '.join(out)).strip()\n",
    "    except:\n",
    "        # Fallback to simple correction\n",
    "        return simple_correct(text, english, freq)\n",
    "\n",
    "def simple_correct(text, english, freq):\n",
    "    \"\"\"Fallback correction without POS tagging\"\"\"\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    out = []\n",
    "    for sent in sent_tokenize(text):\n",
    "        words = []\n",
    "        for w in word_tokenize(sent):\n",
    "            if w in string.punctuation: \n",
    "                words.append(w)\n",
    "                continue\n",
    "            # Basic corrections\n",
    "            w = re.sub(r'0','o',w)\n",
    "            w = re.sub(r'1','l',w)\n",
    "            if w.isalpha() and len(w)>1: \n",
    "                w = spell_check(w,english,freq)\n",
    "            words.append(w)\n",
    "        s = ' '.join(words)\n",
    "        s = re.sub(r'\\s+([,.!?;:])',r'\\1',s)\n",
    "        out.append(s)\n",
    "    return ' '.join(out)\n",
    "\n",
    "# Main extraction with error handling\n",
    "def extract_pdf(pdf_path, out_path):\n",
    "    print(\"Setting up NLTK...\")\n",
    "    nltk_available = setup_nltk()\n",
    "    \n",
    "    if not nltk_available:\n",
    "        print(\"✗ NLTK unavailable, using basic correction\")\n",
    "        return None\n",
    "    \n",
    "    print(\"✓ Creating word dictionaries...\")\n",
    "    english, freq = create_dict()\n",
    "    \n",
    "    if not english:\n",
    "        print(\"✗ Could not load word dictionaries\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✓ Loaded {len(english)} words\")\n",
    "    \n",
    "    text_blocks, img_blocks = [], []\n",
    "    \n",
    "    # Extract native text\n",
    "    print(\"Extracting text with pdfplumber...\")\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, p in enumerate(pdf.pages, 1):\n",
    "                t = p.extract_text()\n",
    "                if t and t.strip(): \n",
    "                    text_blocks.append(f\"=== PAGE {i} - TEXT ===\\n{t}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pdfplumber: {e}\")\n",
    "    \n",
    "    # Extract and OCR images\n",
    "    print(\"Processing images...\")\n",
    "    try:\n",
    "        pdf_doc = fitz.open(pdf_path)\n",
    "        for i, page in enumerate(pdf_doc, 1):\n",
    "            images = page.get_images(full=True)\n",
    "            for j, img in enumerate(images, 1):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base = pdf_doc.extract_image(xref)[\"image\"]\n",
    "                    pil = Image.open(io.BytesIO(base)).convert('RGB')\n",
    "                    \n",
    "                    # Enhanced image processing\n",
    "                    pil = ImageEnhance.Contrast(pil).enhance(1.5)\n",
    "                    pil = ImageEnhance.Sharpness(pil).enhance(2.0)\n",
    "                    \n",
    "                    img_cv = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "                    raw = pytesseract.image_to_string(img_cv, config='--oem 3 --psm 6 -l eng').strip()\n",
    "                    \n",
    "                    if raw:\n",
    "                        print(f\"Processing image {j} on page {i}...\")\n",
    "                        corr = correct_text(raw, english, freq)\n",
    "                        img_blocks.append(f\"=== PAGE {i} - IMAGE {j} ===\\nRAW OCR:\\n{raw}\\n\\nCORRECTED (NLTK):\\n{corr}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {j} on page {i}: {e}\")\n",
    "                    continue\n",
    "        pdf_doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image processing: {e}\")\n",
    "    \n",
    "    # Combine and save\n",
    "    os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)\n",
    "    \n",
    "    all_content = text_blocks.copy()\n",
    "    if img_blocks:\n",
    "        all_content.extend([\"\\n\" + \"=\"*60, \"CONTENT FROM IMAGES (NLTK CORRECTED)\", \"=\"*60])\n",
    "        all_content.extend(img_blocks)\n",
    "    \n",
    "    final = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    with open(out_path, 'w', encoding='utf-8') as f: \n",
    "        f.write(final)\n",
    "    \n",
    "    print(f\"✓ Saved results to: {out_path}\")\n",
    "    return final\n",
    "\n",
    "# Test the enhanced version\n",
    "if __name__==\"__main__\":\n",
    "    input_pdf = \"./Tests files/MC test.pdf\"\n",
    "    output_file = \"./Tests output/enhanced_nltk_corrected.txt\"\n",
    "    \n",
    "    print(\"Testing Enhanced Compact OCR with NLTK...\")\n",
    "    result = extract_pdf(input_pdf, output_file)\n",
    "    \n",
    "    if result:\n",
    "        print(\"✓ Enhanced extraction complete!\")\n",
    "        print(\"✓ Features: POS tagging, context-aware correction, enhanced image processing\")\n",
    "        print(f\"✓ Preview:\\n{result[:400]}...\")\n",
    "    else:\n",
    "        print(\"✗ Extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c579aba",
   "metadata": {},
   "source": [
    "## Frontend Integration Pipeline - JSON Output for Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43028c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting OCR Pipeline...\n",
      "✅ Extraction successful!\n",
      "   📄 6 pages processed\n",
      "   🖼️  18 images processed\n",
      "   📝 1994 words extracted\n",
      "   💾 JSON saved: MC test_processed.json\n",
      "✅ Extraction successful!\n",
      "   📄 6 pages processed\n",
      "   🖼️  18 images processed\n",
      "   📝 1994 words extracted\n",
      "   💾 JSON saved: MC test_processed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import pdfplumber\n",
    "import fitz\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import nltk\n",
    "\n",
    "class OCRPipeline:\n",
    "    def __init__(self):\n",
    "        self.setup_nltk()\n",
    "        self.english_words, self.word_freq, self.stop_words = self.create_dict()\n",
    "        \n",
    "    def setup_nltk(self):\n",
    "        \"\"\"Setup NLTK with quiet initialization\"\"\"\n",
    "        try:\n",
    "            nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "            if nltk_data_path not in nltk.data.path:\n",
    "                nltk.data.path.append(nltk_data_path)\n",
    "            \n",
    "            required_data = ['punkt', 'words', 'averaged_perceptron_tagger', 'brown', 'stopwords']\n",
    "            missing = []\n",
    "            \n",
    "            for data in required_data:\n",
    "                try:\n",
    "                    nltk.data.find(f'tokenizers/{data}' if data == 'punkt' else \n",
    "                                  f'taggers/{data}' if 'tagger' in data else f'corpora/{data}')\n",
    "                except LookupError:\n",
    "                    missing.append(data)\n",
    "            \n",
    "            for data in missing:\n",
    "                nltk.download(data, download_dir=nltk_data_path, quiet=True)\n",
    "                \n",
    "            self.nltk_available = True\n",
    "        except Exception as e:\n",
    "            self.nltk_available = False\n",
    "    \n",
    "    def create_dict(self):\n",
    "        \"\"\"Create word dictionaries and stopwords for text processing\"\"\"\n",
    "        try:\n",
    "            if not self.nltk_available:\n",
    "                return set(), Counter(), set()\n",
    "            \n",
    "            from nltk.corpus import brown, words, stopwords\n",
    "            english_words = set(words.words())\n",
    "            brown_words = [w.lower() for w in brown.words() if w.isalpha()]\n",
    "            word_freq = Counter(brown_words)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            return english_words, word_freq, stop_words\n",
    "        except:\n",
    "            return set(), Counter(), set()\n",
    "    \n",
    "    def edit_distance(self, s1, s2):\n",
    "        \"\"\"Calculate edit distance between strings\"\"\"\n",
    "        if len(s1) < len(s2): \n",
    "            return self.edit_distance(s2, s1)\n",
    "        if not s2: \n",
    "            return len(s1)\n",
    "        \n",
    "        prev = list(range(len(s2) + 1))\n",
    "        for i, c1 in enumerate(s1):\n",
    "            curr = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                ins, dele, sub = prev[j+1]+1, curr[j]+1, prev[j]+(c1!=c2)\n",
    "                curr.append(min(ins, dele, sub))\n",
    "            prev = curr\n",
    "        return prev[-1]\n",
    "    \n",
    "    def spell_check(self, word, max_d=2):\n",
    "        \"\"\"Spell check using edit distance and frequency\"\"\"\n",
    "        word = word.lower()\n",
    "        if not self.english_words or word in self.english_words:\n",
    "            return word\n",
    "        \n",
    "        candidates = []\n",
    "        for w in self.english_words:\n",
    "            if abs(len(w) - len(word)) <= max_d:\n",
    "                dist = self.edit_distance(word, w)\n",
    "                if dist <= max_d:\n",
    "                    candidates.append((w, dist, self.word_freq.get(w, 1)))\n",
    "        \n",
    "        if candidates:\n",
    "            return sorted(candidates, key=lambda x: (x[1], -x[2]))[0][0]\n",
    "        return word\n",
    "    \n",
    "    def correct_text(self, text):\n",
    "        \"\"\"Advanced text correction with POS tagging\"\"\"\n",
    "        if not self.nltk_available or not text.strip():\n",
    "            return self.simple_correct(text)\n",
    "        \n",
    "        try:\n",
    "            from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "            from nltk.tag import pos_tag\n",
    "            \n",
    "            sentences = sent_tokenize(text)\n",
    "            corrected_sentences = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                words = word_tokenize(sentence)\n",
    "                pos_tags = pos_tag(words)\n",
    "                corrected = []\n",
    "                \n",
    "                for word, pos in pos_tags:\n",
    "                    if word in string.punctuation:\n",
    "                        corrected.append(word)\n",
    "                        continue\n",
    "                    \n",
    "                    # Context-aware corrections\n",
    "                    if pos.startswith('NN'):  # Nouns\n",
    "                        word = re.sub(r'rn', 'm', word)\n",
    "                        word = re.sub(r'cl', 'd', word)\n",
    "                    elif pos.startswith('VB'):  # Verbs\n",
    "                        word = re.sub(r'1ng', 'ing', word)\n",
    "                    elif pos.startswith('DT'):  # Determiners\n",
    "                        if word.lower() in ['tlle', 'tl1e']:\n",
    "                            word = 'the'\n",
    "                    \n",
    "                    # OCR fixes\n",
    "                    if any(c.isdigit() for c in word) and any(c.isalpha() for c in word):\n",
    "                        word = re.sub(r'0', 'o', word)\n",
    "                        word = re.sub(r'1', 'l', word)\n",
    "                        word = re.sub(r'5', 'S', word)\n",
    "                        word = re.sub(r'8', 'B', word)\n",
    "                    \n",
    "                    # Spell check\n",
    "                    if word.isalpha() and len(word) > 1:\n",
    "                        word = self.spell_check(word)\n",
    "                    \n",
    "                    corrected.append(word)\n",
    "                \n",
    "                # Reconstruct sentence\n",
    "                s = ' '.join(corrected)\n",
    "                s = re.sub(r'\\s+([,.!?;:])', r'\\1', s)\n",
    "                s = re.sub(r'([,.!?;:])\\s*([A-Za-z])', r'\\1 \\2', s)\n",
    "                corrected_sentences.append(s)\n",
    "            \n",
    "            return re.sub(r'\\s+', ' ', ' '.join(corrected_sentences)).strip()\n",
    "        except:\n",
    "            return self.simple_correct(text)\n",
    "    \n",
    "    def simple_correct(self, text):\n",
    "        \"\"\"Fallback correction without advanced features\"\"\"\n",
    "        corrections = [\n",
    "            (r'rn', 'm'), (r'cl', 'd'), (r'vv', 'w'),\n",
    "            (r'(\\w)1(\\w)', r'\\1l\\2'), (r'(\\w)0(\\w)', r'\\1o\\2'),\n",
    "            (r'\\s+', ' '), (r'\\btlle\\b', 'the'), (r'\\btl1e\\b', 'the'),\n",
    "            (r'\\s+([,.!?;:])', r'\\1')\n",
    "        ]\n",
    "        \n",
    "        corrected = text\n",
    "        for pattern, replacement in corrections:\n",
    "            corrected = re.sub(pattern, replacement, corrected)\n",
    "        return corrected.strip()\n",
    "    \n",
    "    def process_normal_text(self, text):\n",
    "        \"\"\"Process normal PDF text with NLTK cleaning and stopword filtering\"\"\"\n",
    "        if not text.strip():\n",
    "            return text.strip()\n",
    "        \n",
    "        try:\n",
    "            # Clean up common PDF extraction issues\n",
    "            text = re.sub(r'\\s+', ' ', text)  # Multiple spaces\n",
    "            text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # CamelCase splitting\n",
    "            text = re.sub(r'(\\w)([.!?])', r'\\1\\2 ', text)  # Punctuation spacing\n",
    "            \n",
    "            if not self.nltk_available:\n",
    "                return text.strip()\n",
    "            \n",
    "            from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "            \n",
    "            # Sentence tokenization and cleaning\n",
    "            sentences = sent_tokenize(text)\n",
    "            cleaned_sentences = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Remove very short sentences (likely extraction errors)\n",
    "                if len(sentence.split()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Remove excessive stopwords from sentence\n",
    "                words = word_tokenize(sentence.lower())\n",
    "                \n",
    "                # Keep sentence structure but filter out excessive stopwords\n",
    "                # Only remove if more than 60% of words are stopwords\n",
    "                content_words = [w for w in words if w not in self.stop_words and w.isalpha()]\n",
    "                if len(content_words) > len(words) * 0.4:  # At least 40% content words\n",
    "                    cleaned_sentences.append(sentence.strip())\n",
    "            \n",
    "            return ' '.join(cleaned_sentences)\n",
    "        except:\n",
    "            return text.strip()\n",
    "    \n",
    "    def analyze_content(self, text):\n",
    "        \"\"\"AI-like content analysis for frontend\"\"\"\n",
    "        if not text.strip():\n",
    "            return {\n",
    "                \"concepts\": [],\n",
    "                \"difficulty\": \"Unknown\",\n",
    "                \"word_count\": 0,\n",
    "                \"estimated_reading_time\": 0,\n",
    "                \"key_topics\": [],\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "        \n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Simple concept extraction (can be enhanced with NLP models)\n",
    "        concepts = []\n",
    "        key_terms = []\n",
    "        \n",
    "        # Look for academic/technical terms\n",
    "        if self.nltk_available:\n",
    "            try:\n",
    "                from nltk.tokenize import word_tokenize\n",
    "                from nltk.tag import pos_tag\n",
    "                \n",
    "                tokens = word_tokenize(text.lower())\n",
    "                pos_tags = pos_tag(tokens)\n",
    "                \n",
    "                # Extract nouns as potential concepts (excluding stopwords)\n",
    "                nouns = [word for word, pos in pos_tags \n",
    "                        if pos.startswith('NN') and len(word) > 3 \n",
    "                        and word not in self.stop_words]\n",
    "                noun_freq = Counter(nouns)\n",
    "                concepts = [word.title() for word, freq in noun_freq.most_common(8) if freq > 1]\n",
    "                \n",
    "                # Extract proper nouns as key topics\n",
    "                proper_nouns = [word for word, pos in pos_tags \n",
    "                              if pos == 'NNP' and len(word) > 2]\n",
    "                key_topics = list(set(proper_nouns))[:5]\n",
    "                \n",
    "            except:\n",
    "                # Fallback to simple word analysis\n",
    "                words_clean = [w.lower().strip('.,!?;:') for w in words \n",
    "                             if len(w) > 4 and w.lower() not in self.stop_words]\n",
    "                word_freq = Counter(words_clean)\n",
    "                concepts = [w.title() for w, f in word_freq.most_common(6)]\n",
    "                key_topics = concepts[:3]\n",
    "        else:\n",
    "            # Simple analysis without NLTK\n",
    "            basic_stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'this', 'that', 'these', 'those'}\n",
    "            words_clean = [w.lower().strip('.,!?;:') for w in words \n",
    "                         if len(w) > 4 and w.lower() not in basic_stopwords]\n",
    "            word_freq = Counter(words_clean)\n",
    "            concepts = [w.title() for w, f in word_freq.most_common(6)]\n",
    "            key_topics = concepts[:3]\n",
    "        \n",
    "        # Estimate difficulty based on word complexity\n",
    "        complex_words = [w for w in words if len(w) > 8]\n",
    "        complexity_ratio = len(complex_words) / max(word_count, 1)\n",
    "        \n",
    "        if complexity_ratio > 0.15:\n",
    "            difficulty = \"Advanced\"\n",
    "        elif complexity_ratio > 0.08:\n",
    "            difficulty = \"Intermediate\"\n",
    "        else:\n",
    "            difficulty = \"Beginner\"\n",
    "        \n",
    "        # Reading time (average 200 words per minute)\n",
    "        reading_time = max(1, round(word_count / 200))\n",
    "        \n",
    "        # Confidence score based on text quality\n",
    "        confidence_score = min(1.0, max(0.1, (word_count / 100) * 0.8 + (len(concepts) / 10) * 0.2))\n",
    "        \n",
    "        return {\n",
    "            \"concepts\": concepts,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"word_count\": word_count,\n",
    "            \"estimated_reading_time\": reading_time,\n",
    "            \"key_topics\": key_topics,\n",
    "            \"confidence_score\": round(confidence_score, 2)\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_path, user_id=None):\n",
    "        \"\"\"Main processing pipeline for frontend integration\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "            # Process based on file type\n",
    "            file_ext = os.path.splitext(file_path)[1].lower()\n",
    "            \n",
    "            if file_ext == '.pdf':\n",
    "                result = self.process_pdf(file_path)\n",
    "            elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "                result = self.process_image(file_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
    "            \n",
    "            # Analyze content\n",
    "            all_text = result['extracted_text']\n",
    "            analysis = self.analyze_content(all_text)\n",
    "            \n",
    "            # Create Firebase-ready JSON structure (Firebase will add timestamp and ID)\n",
    "            firebase_data = {\n",
    "                \"user_id\": user_id or \"anonymous\",\n",
    "                \"file_info\": {\n",
    "                    \"original_name\": os.path.basename(file_path),\n",
    "                    \"file_type\": file_ext,\n",
    "                    \"file_size\": os.path.getsize(file_path),\n",
    "                    \"processing_method\": result['processing_method']\n",
    "                },\n",
    "                \"extraction_results\": {\n",
    "                    \"raw_text\": result['raw_text'],\n",
    "                    \"corrected_text\": result['corrected_text'],\n",
    "                    \"extracted_text\": all_text,\n",
    "                    \"pages_processed\": result['pages_processed'],\n",
    "                    \"images_processed\": result['images_processed']\n",
    "                },\n",
    "                \"ai_analysis\": analysis,\n",
    "                \"processing_metadata\": {\n",
    "                    \"nltk_available\": self.nltk_available,\n",
    "                    \"processing_time\": result.get('processing_time', 0),\n",
    "                    \"corrections_applied\": result.get('corrections_applied', 0)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return firebase_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return error structure for Firebase\n",
    "            return {\n",
    "                \"user_id\": user_id or \"anonymous\",\n",
    "                \"error\": True,\n",
    "                \"error_message\": str(e),\n",
    "                \"file_info\": {\n",
    "                    \"original_name\": os.path.basename(file_path) if os.path.exists(file_path) else \"unknown\",\n",
    "                    \"file_type\": \"unknown\",\n",
    "                    \"processing_method\": \"failed\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF file with hybrid approach\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        text_blocks = []\n",
    "        image_blocks = []\n",
    "        raw_texts = []\n",
    "        corrected_texts = []\n",
    "        corrections_count = 0\n",
    "        \n",
    "        # Extract native text with pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text = page.extract_text() or ''\n",
    "                    if text.strip():\n",
    "                        # Process normal text with NLTK cleaning\n",
    "                        cleaned_text = self.process_normal_text(text)\n",
    "                        text_blocks.append({\n",
    "                            \"page\": page_num,\n",
    "                            \"type\": \"native_text\",\n",
    "                            \"content\": cleaned_text\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error with pdfplumber: {e}\")\n",
    "        \n",
    "        # Extract and OCR images\n",
    "        try:\n",
    "            pdf_doc = fitz.open(pdf_path)\n",
    "            for page_num, page in enumerate(pdf_doc, 1):\n",
    "                images = page.get_images(full=True)\n",
    "                for img_idx, img_info in enumerate(images, 1):\n",
    "                    try:\n",
    "                        xref = img_info[0]\n",
    "                        base_img = pdf_doc.extract_image(xref)\n",
    "                        image_bytes = base_img[\"image\"]\n",
    "                        \n",
    "                        # Process image\n",
    "                        pil_img = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "                        pil_img = ImageEnhance.Contrast(pil_img).enhance(1.5)\n",
    "                        pil_img = ImageEnhance.Sharpness(pil_img).enhance(2.0)\n",
    "                        \n",
    "                        img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "                        raw_text = pytesseract.image_to_string(img_cv, config='--oem 3 --psm 6 -l eng').strip()\n",
    "                        \n",
    "                        if raw_text:\n",
    "                            corrected_text = self.correct_text(raw_text)\n",
    "                            corrections_count += len(raw_text.split()) - len(corrected_text.split())\n",
    "                            \n",
    "                            raw_texts.append(raw_text)\n",
    "                            corrected_texts.append(corrected_text)\n",
    "                            \n",
    "                            image_blocks.append({\n",
    "                                \"page\": page_num,\n",
    "                                \"image\": img_idx,\n",
    "                                \"type\": \"ocr_text\",\n",
    "                                \"raw_content\": raw_text,\n",
    "                                \"corrected_content\": corrected_text\n",
    "                            })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            pdf_doc.close()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error with image processing: {e}\")\n",
    "        \n",
    "        # Combine all text\n",
    "        all_text_parts = []\n",
    "        for block in text_blocks:\n",
    "            all_text_parts.append(block['content'])\n",
    "        for block in image_blocks:\n",
    "            all_text_parts.append(block['corrected_content'])\n",
    "        \n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"processing_method\": \"hybrid_pdf\",\n",
    "            \"extracted_text\": \"\\n\\n\".join(all_text_parts),\n",
    "            \"raw_text\": \"\\n\\n\".join(raw_texts),\n",
    "            \"corrected_text\": \"\\n\\n\".join(corrected_texts),\n",
    "            \"pages_processed\": len(set([b['page'] for b in text_blocks + image_blocks])),\n",
    "            \"images_processed\": len(image_blocks),\n",
    "            \"processing_time\": round(processing_time, 2),\n",
    "            \"corrections_applied\": abs(corrections_count),\n",
    "            \"detailed_blocks\": text_blocks + image_blocks\n",
    "        }\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Process single image file\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Load and enhance image\n",
    "            pil_img = Image.open(image_path).convert('RGB')\n",
    "            pil_img = ImageEnhance.Contrast(pil_img).enhance(1.5)\n",
    "            pil_img = ImageEnhance.Sharpness(pil_img).enhance(2.0)\n",
    "            \n",
    "            # Convert to OpenCV format\n",
    "            img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # OCR\n",
    "            raw_text = pytesseract.image_to_string(img_cv, config='--oem 3 --psm 6 -l eng').strip()\n",
    "            corrected_text = self.correct_text(raw_text) if raw_text else \"\"\n",
    "            \n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            corrections_count = len(raw_text.split()) - len(corrected_text.split()) if raw_text and corrected_text else 0\n",
    "            \n",
    "            return {\n",
    "                \"processing_method\": \"image_ocr\",\n",
    "                \"extracted_text\": corrected_text,\n",
    "                \"raw_text\": raw_text,\n",
    "                \"corrected_text\": corrected_text,\n",
    "                \"pages_processed\": 1,\n",
    "                \"images_processed\": 1,\n",
    "                \"processing_time\": round(processing_time, 2),\n",
    "                \"corrections_applied\": abs(corrections_count)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {e}\")\n",
    "\n",
    "# Pipeline usage functions for frontend integration\n",
    "def process_file_for_frontend(file_path, user_id=None, output_dir=\"./firebase_data\"):\n",
    "    \"\"\"\n",
    "    Main function to be called from frontend\n",
    "    Returns JSON data ready for Firebase storage\n",
    "    \"\"\"\n",
    "    pipeline = OCRPipeline()\n",
    "    result = pipeline.process_file(file_path, user_id)\n",
    "    \n",
    "    # Save JSON file for Firebase upload (Firebase will generate document ID and timestamp)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use filename-based naming since Firebase will handle document IDs\n",
    "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    json_filename = f\"{filename}_processed.json\"\n",
    "    json_path = os.path.join(output_dir, json_filename)\n",
    "    \n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return result, json_path\n",
    "\n",
    "# Test the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    test_file = \"./Tests files/MC test.pdf\"\n",
    "    test_user = \"user_123\"\n",
    "    \n",
    "    print(\"🚀 Starting OCR Pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        firebase_data, json_file = process_file_for_frontend(test_file, test_user)\n",
    "        \n",
    "        # Simple success output\n",
    "        print(f\"✅ Extraction successful!\")\n",
    "        print(f\"   📄 {firebase_data['extraction_results']['pages_processed']} pages processed\")\n",
    "        print(f\"   🖼️  {firebase_data['extraction_results']['images_processed']} images processed\") \n",
    "        print(f\"   📝 {firebase_data['ai_analysis']['word_count']} words extracted\")\n",
    "        print(f\"   💾 JSON saved: {os.path.basename(json_file)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed0eee",
   "metadata": {},
   "source": [
    "## Simple Flask Server for Frontend Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e627bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Flask server to integrate OCR pipeline with React frontend\n",
    "# Run this in a separate terminal: python ocr_server.py\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for React frontend\n",
    "\n",
    "@app.route('/api/ocr/process', methods=['POST'])\n",
    "def process_ocr():\n",
    "    try:\n",
    "        # Check if file is in request\n",
    "        if 'file' not in request.files:\n",
    "            return jsonify({'error': 'No file provided'}), 400\n",
    "        \n",
    "        file = request.files['file']\n",
    "        if file.filename == '':\n",
    "            return jsonify({'error': 'No file selected'}), 400\n",
    "        \n",
    "        # Get user_id from form data (optional)\n",
    "        user_id = request.form.get('user_id', 'anonymous')\n",
    "        \n",
    "        # Create temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:\n",
    "            file.save(temp_file.name)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Process file using our OCR pipeline\n",
    "            result, json_path = process_file_for_frontend(temp_path, user_id)\n",
    "            \n",
    "            # Return the processed data\n",
    "            response_data = {\n",
    "                'success': True,\n",
    "                'data': result,\n",
    "                'message': 'File processed successfully'\n",
    "            }\n",
    "            \n",
    "            return jsonify(response_data)\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            if os.path.exists(temp_path):\n",
    "                os.unlink(temp_path)\n",
    "                \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e), 'success': False}), 500\n",
    "\n",
    "@app.route('/api/ocr/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({'status': 'healthy', 'message': 'OCR service is running'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"🚀 Starting OCR Server...\")\n",
    "    print(\"📡 Server will run on http://localhost:5000\")\n",
    "    print(\"🔗 Frontend can send POST requests to http://localhost:5000/api/ocr/process\")\n",
    "    app.run(debug=True, port=5000)\n",
    "\n",
    "# To run this server:\n",
    "# 1. Save this cell content to a file named 'ocr_server.py'\n",
    "# 2. Install Flask: pip install flask flask-cors\n",
    "# 3. Run: python ocr_server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conceptify - AI Powered Learning Platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
