{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2db92",
   "metadata": {},
   "source": [
    "# OCR Test with Tesseract\n",
    "\n",
    "## Requirements\n",
    "1. **Install Tesseract OCR**: \n",
    "   - Install Tesseract: `winget install --id UB-Mannheim.TesseractOCR` \n",
    "   - add it to system varibales \n",
    "2. **Python packages**: `uv pip install opencv-python pytesseract pillow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b24cd",
   "metadata": {},
   "source": [
    "## Only Pdf (textual format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e4df974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./Tests output/output_from_text.txt\n",
      "\n",
      "\n",
      "Extracted text:\n",
      " Experiment-07\n",
      "Roll No: A3-754\n",
      "Aim: To study & implement Part-of-Speech (POS) tagging using the Viterbi Algorithm in Hidden Markov\n",
      "Models (HMM)\n",
      "Theory:\n",
      "Part-of-Speech (POS) tagging is one of the most fundamental tasks in Natural Language Processing (NLP). It\n",
      "refers to the process of assigning a grammatical category, such as noun, verb, adjective, adverb, etc., to each\n",
      "word in a sentence. For example, in the sentence “Fish can swim”, the word “Fish” can be a noun, “can”\n",
      "can be a verb, and “swim” is also a verb. However, words are often ambiguous – for example, “can” can also\n",
      "mean a noun (like “a can of soda”). Hence, simply looking at individual words is not enough; we need to\n",
      "consider the context of the words in the sentence.\n",
      "➢ Hidden Markov Model (HMM) for POS Tagging\n",
      "A Hidden Markov Model is a statistical model in which the system being modeled is assumed to be a Markov\n",
      "process with hidden states. In the context of POS tagging:\n",
      "• The hidden states are the POS tags (e.g., noun, verb, adjective).\n",
      "• The observations are the actual words in the sentence.\n",
      "• Initial probabilities define how likely it is for a sentence to start with a particular tag.\n",
      "• Transition probabilities represent the likelihood of moving from one tag to another (for example, a\n",
      "noun is more likely followed by a verb than by another noun).\n",
      "• Emission probabilities define the likelihood of a word being generated by a particular tag (for\n",
      "example, the word “swim” is much more likely to be generated by a verb tag than by a noun tag).\n",
      "Thus, POS tagging with HMMs is about finding the sequence of hidden tags (states) that most likely explains\n",
      "the observed sequence of words.\n",
      "➢ The Challenge of Ambiguity\n",
      "Language is inherently ambiguous. For example:\n",
      "• The word “fish” can be a noun (“A fish is swimming”) or a verb (“They fish at the lake”).\n",
      "• The word “can” can be a noun (“A can of soda”) or a verb (“I can do it”).\n",
      "This ambiguity means that we cannot simply assign tags based only on dictionary definitions. Instead, we\n",
      "must consider probabilities and context.\n",
      "This is where the Viterbi algorithm is useful: it can efficiently compute the most probable sequence of tags\n",
      "for the entire sentence, rather than tagging each word independently.\n",
      "➢ The Viterbi Algorithm\n",
      "The Viterbi Algorithm is a dynamic programming algorithm that is widely used to find the most likely\n",
      "sequence of hidden states in Hidden Markov Models. It avoids brute-force enumeration (which would be\n",
      "exponential in complexity) by breaking the problem into subproblems and reusing results.\n",
      "The algorithm proceeds in three major steps:\n",
      "Step 1: Initialization:\n",
      "a. For the first word in the sentence, compute the probability of each possible tag by multiplying its\n",
      "initial probability with its emission probability for the word.\n",
      "b. This initializes the first column of the Viterbi matrix.\n",
      "Step 2: Recursion (Forward Pass):\n",
      "a. For each subsequent word, calculate the probability of reaching each tag from all possible previous\n",
      "tags.\n",
      "b. The probability is computed as:\n",
      "𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 × 𝑡𝑟𝑎𝑛𝑠𝑖𝑡𝑖𝑜𝑛 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 × 𝑒𝑚𝑖𝑠𝑠𝑖𝑜𝑛 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦\n",
      "c. For each tag, keep track of the maximum probability and also store which previous tag gave this\n",
      "maximum.\n",
      "d. This step continues until the last word is processed.\n",
      "Step 3: Backtracking:\n",
      "a. After filling the matrix, the highest probability in the last column represents the end of the most\n",
      "likely tag sequence.\n",
      "b. By tracing back the stored pointers, we can reconstruct the full sequence of POS tags.\n",
      "This way, the algorithm produces not only the best sequence of tags but also the probability of that sequence.\n",
      "➢ Advantages of Using Viterbi for POS Tagging\n",
      "1. Efficiency: The Viterbi algorithm uses dynamic programming, which drastically reduces computation\n",
      "compared to brute force approaches.\n",
      "2. Accuracy: By considering both emission and transition probabilities, the algorithm makes decisions\n",
      "based on context, which improves tagging accuracy.\n",
      "3. Scalability: It can handle sentences of varying lengths efficiently.\n",
      "4. Probabilistic framework: Instead of making hard-coded rules, it relies on data-driven probabilities,\n",
      "which makes it adaptable to different languages and corpora.\n",
      "➢ Applications of POS Tagging\n",
      "POS tagging has wide applications in various areas of Natural Language Processing, including:\n",
      "• Speech recognition: Understanding context to distinguish between homophones.\n",
      "• Machine translation: Ensuring correct grammar and word order in translated sentences.\n",
      "• Information retrieval: Improving search engine results by understanding query semantics.\n",
      "• Text-to-speech systems: Determining correct pronunciations based on word roles.\n",
      "• Question answering systems: Extracting correct answers from text based on grammatical structure.\n",
      "➢ Illustrative Example\n",
      "Let us consider the sentence: “Fish can swim.”\n",
      "• Possible tags: Noun, Verb, Adjective.\n",
      "• Initial probabilities: Define which tags are more likely at the start.\n",
      "• Transition probabilities: Define how likely each tag is to follow another (e.g., a noun is often followed\n",
      "by a verb).\n",
      "• Emission probabilities: Define how likely a word belongs to a certain tag (e.g., “swim” is more\n",
      "probable as a verb).\n",
      "Step by step, the Viterbi algorithm builds a probability table and finally determines that the best sequence of\n",
      "tags is:\n",
      "“Fish” → Noun\n",
      "“can” → Verb\n",
      "“swim” → Verb\n",
      "This matches our intuitive understanding of the sentence.\n",
      "The Viterbi algorithm provides an efficient and robust solution for POS tagging in Natural Language\n",
      "Processing. By combining Hidden Markov Models with dynamic programming, it overcomes the problem of\n",
      "ambiguity in language and finds the most probable sequence of grammatical tags for a sentence. This forms\n",
      "the basis of many modern NLP systems and is an important concept in the study of artificial intelligence and\n",
      "computational linguistics.\n",
      "Program:\n",
      "import numpy as np\n",
      "def viterbi_pos_tagging(sentence, tags, init_probs, trans_probs, emit_probs):\n",
      "T = len(sentence)\n",
      "N = len(tags)\n",
      "viterbi = np.full((N, T), -np.inf)\n",
      "backpointer = np.zeros((N, T), dtype=int)\n",
      "# Initialization\n",
      "for i, tag in enumerate(tags):\n",
      "viterbi[i, 0] = np.log(init_probs.get(tag, 1e-10)) +\n",
      "np.log(emit_probs.get((tag, sentence[0]), 1e-10))\n",
      "# Recursion\n",
      "for t in range(1, T):\n",
      "for j, curr_tag in enumerate(tags):\n",
      "probs = [viterbi[i, t-1] + np.log(trans_probs.get((tags[i],\n",
      "curr_tag), 1e-10)) + np.log(emit_probs.get((curr_tag, sentence[t]), 1e-10)) for\n",
      "i in range(N)]\n",
      "backpointer[j, t] = np.argmax(probs)\n",
      "viterbi[j, t] = np.max(probs)\n",
      "# Backtracking\n",
      "best_path = [tags[np.argmax(viterbi[:, T-1])]]\n",
      "for t in range(T-1, 0, -1):\n",
      "best_path.insert(0, tags[backpointer[tags.index(best_path[0]), t]])\n",
      "return best_path, np.exp(np.max(viterbi[:, T-1]))\n",
      "# Example usage\n",
      "tags = ['Noun', 'Verb', 'Adj']\n",
      "init_probs = {'Noun': 0.6, 'Verb': 0.3, 'Adj': 0.1}\n",
      "trans_probs = {('Noun', 'Verb'): 0.4, ('Noun', 'Noun'): 0.5, ('Noun', 'Adj'):\n",
      "0.1,\n",
      "('Verb', 'Verb'): 0.2, ('Verb', 'Noun'): 0.6, ('Verb', 'Adj'):\n",
      "0.2,\n",
      "('Adj', 'Noun'): 0.8, ('Adj', 'Verb'): 0.1, ('Adj', 'Adj'): 0.1}\n",
      "emit_probs = {('Noun', 'fish'): 0.5, ('Verb', 'fish'): 0.2, ('Adj', 'fish'):\n",
      "0.1,\n",
      "('Noun', 'can'): 0.1, ('Verb', 'can'): 0.4, ('Adj', 'can'): 0.1,\n",
      "('Noun', 'swim'): 0.05, ('Verb', 'swim'): 0.6, ('Adj', 'swim'):\n",
      "0.01}\n",
      "sentence = ['fish', 'can', 'swim']\n",
      "best_tags, best_prob = viterbi_pos_tagging(sentence, tags, init_probs,\n",
      "trans_probs, emit_probs)\n",
      "print(\"Sentence:\", sentence)\n",
      "print(\"Most likely tag sequence:\", best_tags)\n",
      "print(\"Probability of tag sequence:\", best_prob)\n",
      "Output:\n",
      "Conclusion:\n",
      "In this experiment, we studied and implemented Part-of-Speech tagging using the Viterbi Algorithm with a\n",
      "Hidden Markov Model.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def extract_pdf(input_pdf: str, output_txt: str):\n",
    "    if not os.path.exists(input_pdf):\n",
    "        raise FileNotFoundError(f'Input PDF not found: {input_pdf}')\n",
    "\n",
    "    out_dir = os.path.dirname(output_txt) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pages = []\n",
    "    with pdfplumber.open(input_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            pages.append(page.extract_text() or '')\n",
    "\n",
    "    text = '\\n'.join(pages).strip()\n",
    "\n",
    "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Defaults (change as needed)\n",
    "input_pdf = './Tests files/test_text_only.pdf'\n",
    "output_txt = './Tests output/output_from_text.txt'\n",
    "\n",
    "# Run\n",
    "extracted_text = extract_pdf(input_pdf, output_txt)\n",
    "print(f'Saved to {output_txt}')\n",
    "print('\\n\\nExtracted text:\\n', extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ca817",
   "metadata": {},
   "source": [
    "## Only Image (clear Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38e6c8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./Tests output/output_from_image.txt\n",
      "\n",
      "\n",
      "Extracted text:\n",
      " Experiment-07\n",
      "\n",
      "Roll No: A3-754\n",
      "\n",
      "Aim: To study & implement Part-of-Speech (POS) tagging using the Viterbi Algorithm in Hidden Markov\n",
      "Models (HMM)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "def run_ocr(input_path: str, output_path: str):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f'Input file not found: {input_path}')\n",
    "    \n",
    "    out_dir = os.path.dirname(output_path) or '.'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    img = cv2.imread(input_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f'Could not read image: {input_path}')\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Defaults (change as needed)\n",
    "input_path = './Tests files/test_image_only.png'\n",
    "output_path = './Tests output/output_from_image.txt'\n",
    "\n",
    "# Run\n",
    "text = run_ocr(input_path, output_path)\n",
    "print('Saved to ./Tests output/output_from_image.txt')\n",
    "print('\\n\\nExtracted text:\\n',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d4e32",
   "metadata": {},
   "source": [
    "## Pdf (Mixed photos and text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2150179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text:\n",
      " [IMAGE OCR] wanyandGessathe TRUST\n",
      "\n",
      "RAJIV GANDHI INSTITUTE OF TECHNOLOGY, MUMBAI\n",
      "DEPARTMENT OF COMPUTER ENGINEERING\n",
      "Experiment-07\n",
      "Roll No: A3-754\n",
      "Aim: To study and implement the Random Forest algorithm for classification & regression tasks in ML\n",
      "Theory:\n",
      "Random Forest is an ensemble learning technique that combines multiple decision trees to produce a more \n",
      "accurate and robust model. It reduces overfitting and improves generalization compared to a single decision \n",
      "tree. It is a supervised machine learning algorithm that builds multiple decision trees and merges them to get \n",
      "a more accurate and stable prediction. It can be used for both classification and regression tasks.\n",
      "[IMAGE OCR] Working of Random Forest\n",
      "\n",
      "Tree2 (Aggregation)\n",
      "\n",
      "Tree 3\n",
      "➢ Importance:\n",
      "• Handles high-dimensional data efficiently. \n",
      "• Reduces overfitting by averaging multiple trees. \n",
      "• Works well with large datasets and maintains accuracy even if a large portion of data is missing. \n",
      "• Provides feature importance, helping in identifying significant features in a dataset.\n",
      "[IMAGE OCR] Random Sampling.\n",
      "\n",
      "with R t Train\n",
      "\n",
      "2, € R80%100\n",
      "\n",
      "X € ROXIO LY ye ROXIO =\n",
      "\n",
      "Xq € RMOX100\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] wanyandGessathe TRUST\n",
      "\n",
      "RAJIV GANDHI INSTITUTE OF TECHNOLOGY, MUMBAI\n",
      "DEPARTMENT OF COMPUTER ENGINEERING\n",
      "➢ Working of Random Forest:\n",
      "[IMAGE OCR] Random Forest Simplified\n",
      "\n",
      "i.\n",
      "\n",
      "KARD i KAR.\n",
      "\n",
      "Tree-1 Tree-2 Tree-n\n",
      "Class-A Class-B Class-B\n",
      "Majority-Voting\n",
      "\n",
      "Final-Class\n",
      "• Bootstrapping: Randomly sample the dataset with replacement to create multiple subsets. \n",
      "• Building Trees: For each subset, a decision tree is trained, using a random selection of features at\n",
      "each split.\n",
      "• Aggregation:\n",
      "o For classification: Each tree votes for a class, and the majority vote is taken. \n",
      "o For regression: The average of all tree predictions is used.\n",
      "➢ Advantages:\n",
      "• High accuracy and robustness. \n",
      "• Handles missing values well. \n",
      "• Reduces variance compared to a single decision tree.\n",
      "➢ Disadvantages:\n",
      "• Complex and computationally intensive. \n",
      "• Difficult to interpret compared to a single decision tree.\n",
      "➢ Algorithm / Pseudocode:\n",
      "For i=1 to n (number of trees):\n",
      "• Take a bootstrap sample from the training data. \n",
      "• Train a decision tree on the sample. \n",
      "• At each split, consider a random subset of features. \n",
      "• Aggregate predictions from all trees:\n",
      "o Classification → Majority voting. \n",
      "o Regression → Average prediction.\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] wanyandGessathe TRUST\n",
      "\n",
      "RAJIV GANDHI INSTITUTE OF TECHNOLOGY, MUMBAI\n",
      "DEPARTMENT OF COMPUTER ENGINEERING\n",
      "Program:\n",
      "import pandas as pd \n",
      "import numpy as np \n",
      "import plotly.express as px \n",
      "import plotly.figure_factory as ff \n",
      "from sklearn.datasets import load_iris \n",
      "from sklearn.ensemble import RandomForestClassifier \n",
      "from sklearn.metrics import classification_report, confusion_matrix \n",
      "from sklearn.model_selection import train_test_split \n",
      " \n",
      "iris = load_iris() \n",
      "X = pd.DataFrame(iris.data, columns=iris.feature_names) \n",
      "y = pd.Series(iris.target, name='target') \n",
      " \n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
      "random_state=42) \n",
      " \n",
      "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
      "rf_model.fit(X_train, y_train) \n",
      "y_pred = rf_model.predict(X_test) \n",
      " \n",
      "print(\"Classification Report:\\n\") \n",
      "print(classification_report(y_test, y_pred, target_names=iris.target_names)) \n",
      " \n",
      "cm = confusion_matrix(y_test, y_pred) \n",
      "cm_df = pd.DataFrame(cm, index=iris.target_names, columns=iris.target_names) \n",
      "fig_cm = ff.create_annotated_heatmap( \n",
      "    z=cm_df.values, \n",
      "    x=list(cm_df.columns), \n",
      "    y=list(cm_df.index), \n",
      "    colorscale='Blues', \n",
      "    showscale=True \n",
      ") \n",
      "fig_cm.update_layout( \n",
      "    title_text=\"  Random Forest Confusion Matrix  \", \n",
      "    xaxis_title=\"Predicted\", \n",
      "    yaxis_title=\"Actual\" \n",
      ") \n",
      "fig_cm.show() \n",
      " \n",
      "importances = rf_model.feature_importances_ \n",
      "feat_imp = pd.DataFrame({ \n",
      "    'Feature': X.columns, \n",
      "    'Importance': importances \n",
      "}).sort_values(by='Importance', ascending=False) \n",
      " \n",
      "fig_feat = px.bar( \n",
      "    feat_imp,\n",
      "\n",
      "--- Page Break ---\n",
      "\n",
      "[IMAGE OCR] wanyandGessathe TRUST\n",
      "\n",
      "RAJIV GANDHI INSTITUTE OF TECHNOLOGY, MUMBAI\n",
      "DEPARTMENT OF COMPUTER ENGINEERING\n",
      "x='Importance', \n",
      "    y='Feature', \n",
      "    orientation='h', \n",
      "    color='Importance', \n",
      "    color_continuous_scale='magma', \n",
      "    title=\"  Feature Importance in Random Forest  \" \n",
      ") \n",
      "fig_feat.show() \n",
      " \n",
      "Output:\n",
      "[IMAGE OCR] @ Random Forest Confusion Matrix @\n",
      "\n",
      "atone\n",
      "vargetca 0\n",
      "versicolor °\n",
      "\n",
      "veto\n",
      "\n",
      "Predicted\n",
      "\n",
      "rarector\n",
      "[IMAGE OCR] Feature\n",
      "\n",
      "@ Feature Importance in Random Forest\n",
      "\n",
      "pal ith (om\n",
      "\n",
      "seal length (cm)\n",
      "\n",
      "petal length (om)\n",
      "\n",
      "0.25\n",
      "\n",
      "Importance\n",
      "\n",
      "ons\n",
      "\n",
      "Importance\n",
      "\n",
      "o4\n",
      "Conclusion:\n",
      "In this experiment, we studied and implemented Random forest Algorithm.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_pdf_with_images(pdf_path, out_txt):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    final_text = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        blocks = []\n",
    "\n",
    "        # --- Text blocks (with bbox) ---\n",
    "        for b in page.get_text(\"blocks\"):\n",
    "            x0, y0, x1, y1, text, *_ = b\n",
    "            if text.strip():\n",
    "                blocks.append({\n",
    "                    \"bbox\": (x0, y0, x1, y1),\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": text.strip()\n",
    "                })\n",
    "\n",
    "        # --- Image blocks ---\n",
    "        raw_dict = page.get_text(\"rawdict\")\n",
    "        for block in raw_dict[\"blocks\"]:\n",
    "            if block[\"type\"] == 1:  # image\n",
    "                bbox = block[\"bbox\"]\n",
    "                img = page.get_pixmap(matrix=fitz.Matrix(2, 2), clip=fitz.Rect(bbox))\n",
    "                img_pil = Image.frombytes(\"RGB\", [img.width, img.height], img.samples)\n",
    "\n",
    "                # OCR on image\n",
    "                img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "                ocr_text = pytesseract.image_to_string(img_cv).strip()\n",
    "                if ocr_text:\n",
    "                    blocks.append({\n",
    "                        \"bbox\": bbox,\n",
    "                        \"type\": \"image\",\n",
    "                        \"content\": ocr_text\n",
    "                    })\n",
    "\n",
    "        # --- Sort by layout order ---\n",
    "        blocks.sort(key=lambda b: (round(b[\"bbox\"][1]), round(b[\"bbox\"][0])))\n",
    "\n",
    "        # --- Merge page ---\n",
    "        page_text = []\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == \"image\":\n",
    "                page_text.append(f\"[IMAGE OCR] {b['content']}\")\n",
    "            else:\n",
    "                page_text.append(b[\"content\"])\n",
    "        final_text.append(\"\\n\".join(page_text))\n",
    "\n",
    "    merged_text = \"\\n\\n--- Page Break ---\\n\\n\".join(final_text)\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(merged_text)\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "\n",
    "# Example run\n",
    "pdf_path = \"./Tests files/test_mix.pdf\"\n",
    "out_txt = \"./Tests output/output_from_mix.txt\"\n",
    "\n",
    "text = extract_pdf_with_images(pdf_path, out_txt)\n",
    "print('Extracted text:\\n',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c01af",
   "metadata": {},
   "source": [
    "## Pdf (Mixed photos and text), betterment in photo output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03aea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptify-ai-powered-learning-platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
